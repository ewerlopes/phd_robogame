{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import traceback\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from pandas.tools.plotting import lag_plot, autocorrelation_plot\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.stats import norm, skew, kurtosis\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "import scipy.fftpack as fft\n",
    "\n",
    "from helper_functions import getListOfFiles, getCSV, getStatistics, remap_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring meta-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 34 CSV Files found:\n",
      "\n",
      "[\"_2016-11-23-18-49-13_exp1_Player.csv\", \"_2016-11-23-18-49-13_exp2_Player.csv\", \"_2016-11-23-18-49-13_exp3_Player.csv\", \"_2016-11-23-18-49-13_exp4_Player.csv\", \"_2016-11-23-18-49-13_exp5_Player.csv\", \"_2016-11-24-15-43-37_exp1d_Player.csv\", \"_2016-11-24-15-43-37_exp2d_Player.csv\", \"_2016-11-24-15-43-37_exp3d_Player.csv\", \"_2016-11-24-15-43-37_exp4d_Player.csv\", \"_2016-11-24-15-43-37_exp5d_Player.csv\", \"_2016-11-24-15-43-37_exp6d_Player.csv\", \"_2016-11-24-16-23-29_expa_Player.csv\", \"_2016-11-24-16-23-29_expb_Player.csv\", \"_2016-11-24-16-23-29_expc_Player.csv\", \"_2016-11-24-16-23-29_expd_Player.csv\", \"_2016-11-24-16-48-48_exp1d_Player.csv\", \"_2016-11-24-16-48-48_exp2d_Player.csv\", \"_2016-11-24-16-48-48_exp3d_Player.csv\", \"_2016-11-24-17-15-38_expa_Player.csv\", \"_2016-11-24-17-15-38_expb_Player.csv\", \"_2016-11-24-17-15-38_expc_Player.csv\", \"_2016-11-24-17-40-06_expb_Player.csv\", \"_2016-11-26-15-42-51_exp1d_Player.csv\", \"_2016-11-26-16-05-47_exp1d_Player.csv\", \"_2016-11-26-16-35-21_exp1d_Player.csv\", \"_2016-11-26-16-49-44_exp1d_Player.csv\", \"_2016-11-26-17-15-53_exp2_Player.csv\", \"_2016-11-26-17-15-53_exp3_Player.csv\", \"_2016-11-26-17-15-53_exp4_Player.csv\", \"_2016-11-26-17-15-53_fixed_exp1d_Player.csv\", \"_2016-11-26-17-15-53_fixed_exp2d_Player.csv\", \"_2016-11-26-17-38-21_fixed_exp1d_Player.csv\", \"_2016-11-26-18-36-15_expa_Player.csv\", \"_2016-11-26-18-36-15_expb_Player.csv\"]\n"
     ]
    }
   ],
   "source": [
    "csv_dir = \"./data\"\n",
    "files = getListOfFiles(csv_dir, \".csv\")\n",
    "print \">> {} CSV Files found:\\n\".format(len(files))\n",
    "print json.dumps(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a dictionary containing a remap of the name of the features. This is to easy visualization since ros feature names may be very long and also allow quick decoupled modifications (just edit the variable locally). Also, from it we define a ignore list containing the names we do not want to consider for the analysis. That is done by setting to the ignore list all dictionary keys with empty values. **Here we assume the names are consistent feature name w.r.t. the `csv` data. Otherwise, a `ValueError` exception is likely to be thrown!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of ignored topics: []\n"
     ]
    }
   ],
   "source": [
    "# variable for storing the loaded feature names.\n",
    "feature_name_map = {\n",
    "  \"time\" : \"time\",\n",
    "  \"Control\": \"control\",\n",
    "  \"High_level\": \"high_level\",\n",
    "  \"Expectation\": \"expectation\",\n",
    "  \"Activity\": \"activity\",\n",
    "  \"/kinect_features/.ci\": \"ci\",\n",
    "  \"/kinect_features/.distance\": \"distance\",\n",
    "  \"/kinect_features/.proximity\": \"proximity\",\n",
    "  \"robogame/imu_state.gyro.x\": \"gyroX\",\n",
    "  \"robogame/imu_state.gyro.y\": \"gyroY\",\n",
    "  \"robogame/imu_state.gyro.z\": \"gyroZ\",\n",
    "  \"robogame/imu_state.linear_acc.x\": \"accX\",\n",
    "  \"robogame/imu_state.linear_acc.y\": \"accY\",\n",
    "  \"robogame/imu_state.linear_acc.z\": \"accZ\"\n",
    "}\n",
    "\n",
    "ignore_col_list = [k for k,v in feature_name_map.items() if v is \"\"]\n",
    "print \"List of ignored topics: {}\".format(ignore_col_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-features function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(data):\n",
    "    \"\"\"Calculates mean of the data\"\"\"\n",
    "    if isinstance(data,list):\n",
    "        return [d.mean(axis=0, skipna=True, numeric_only=True) for d in data]\n",
    "    else:\n",
    "        return data.mean(axis=0, skipna=True)\n",
    "\n",
    "\n",
    "def std(data):\n",
    "    \"\"\"Calculates the standard deviation\"\"\"\n",
    "    if isinstance(data,list):\n",
    "        return [d.std(axis=0, skipna=True, numeric_only=True) for d in data]\n",
    "    else:\n",
    "        return data.std(axis=0, skipna=True, numeric_only=True)\n",
    "\n",
    "\n",
    "def max_value(data):\n",
    "    \"\"\" Calculates Largest value in array\"\"\"\n",
    "    if isinstance(data,list):\n",
    "        return [d.max(axis=0, skipna=True, numeric_only=True) for d in data]\n",
    "    else:\n",
    "        return data.max(axis=0, skipna=True, numeric_only=True)\n",
    "\n",
    "    \n",
    "def min_value(data):\n",
    "    \"\"\"Calculates smallest value in array\"\"\"\n",
    "    if isinstance(data,list):\n",
    "        return [d.min(axis=0, skipna=True, numeric_only=True) for d in data]\n",
    "    else:\n",
    "        return data.min(axis=0, skipna=True, numeric_only=True)\n",
    "\n",
    "\n",
    "def mad(data):\n",
    "    \"\"\" Calculates the median absolute deviation\"\"\"\n",
    "    if isinstance(data,list):\n",
    "        l = []\n",
    "        for d in data:\n",
    "            m = {}\n",
    "            for k in data._get_numeric_data():\n",
    "                m[k] = abs(data[k].dropna() - data[k].median())\n",
    "                m[k] = pd.Series(m[k]).median()\n",
    "            l.append(pd.Series(m))\n",
    "        return l\n",
    "    else:\n",
    "        m = {}\n",
    "        for k in data._get_numeric_data():\n",
    "            m[k] = abs(data[k].dropna() - data[k].median())\n",
    "            m[k] = pd.Series(m[k]).median()\n",
    "        return pd.Series(m)\n",
    "\n",
    "\n",
    "def sma(data):\n",
    "    \"\"\"Computes Signal magnitude area.\n",
    "    http://dsp.stackexchange.com/questions/18649/signal-magnitude-area\n",
    "    \"\"\"\n",
    "    if isinstance(data,list):\n",
    "        return [d.sum(axis=0, skipna=True, numeric_only=True) / data.shape[0] for d in data]\n",
    "    else:\n",
    "        return data.sum(axis=0, skipna=True, numeric_only=True) / data.shape[0]\n",
    "\n",
    "\n",
    "def energy(data):\n",
    "    \"\"\"Energy measure. Sum of the squares divided by the number of values.\"\"\"\n",
    "    if isinstance(data,list):\n",
    "        return [d.dropna().apply(lambda x: x**2).mean(axis=0, skipna=True, numeric_only=True) for d in data]\n",
    "    else:\n",
    "        return data.dropna().apply(lambda x: x**2).mean(axis=0, skipna=True, numeric_only=True)\n",
    "\n",
    "\n",
    "def iqr(data):\n",
    "    \"\"\"Calculates the interquartile range\n",
    "    http://stackoverflow.com/questions/23228244/how-do-you-find-the-iqr-in-numpy\n",
    "    \"\"\"\n",
    "    if isinstance(data,np.ndarray):\n",
    "        return np.subtract(*np.percentile(data, [75, 25]))\n",
    "    else:\n",
    "        v = {}\n",
    "        for k in data._get_numeric_data():\n",
    "            v[k] = np.subtract(*np.percentile(data[k].dropna(), [75, 25]))\n",
    "        return pd.Series(v)\n",
    "\n",
    "def entropy(data):\n",
    "    \"\"\"Signal entropy\"\"\"\n",
    "    pass\n",
    "\n",
    "def maxInds(data, n_bins=200, filterMean= True):\n",
    "    \"\"\"Returns the index of the frequency component with largest magnitude\"\"\"\n",
    "    m_indexes = {}\n",
    "    for k in data._get_numeric_data():\n",
    "        c_sig = []\n",
    "        if filterMean:\n",
    "            filtered = data[k].dropna().as_matrix()\n",
    "            mean_sig = np.ones_like(filtered)*np.mean(filtered)\n",
    "            # remove mean of the signal, for better results.\n",
    "            c_sig = data[k].dropna().as_matrix() - mean_sig\n",
    "        freqsig = fft.fft(c_sig,n=n_bins) \n",
    "        half_freq_domain = freqsig[:int(n_bins/2)]\n",
    "        #get index in the freq domain\n",
    "        m_indexes[k] = np.where(np.abs(half_freq_domain)==(max(np.abs(half_freq_domain))))[0][0]\n",
    "    return pd.Series(m_indexes)\n",
    "\n",
    "def meanFreq(data, n_bins=200, filterMean=True):\n",
    "    \"\"\"\n",
    "    Weighted average of the frequency components to obtain a mean frequency\n",
    "    http://luscinia.sourceforge.net/page26/page35/page35.html\n",
    "    \"\"\"\n",
    "    m_freq = {}\n",
    "    for k in data._get_numeric_data():\n",
    "        c_sig = []\n",
    "        if filterMean:\n",
    "            filtered = data[k].dropna().as_matrix()\n",
    "            mean_sig = np.ones_like(filtered)*np.mean(filtered)\n",
    "            # remove mean of the signal, for better results.\n",
    "            c_sig = data[k].dropna().as_matrix() - mean_sig\n",
    "        freqsig = fft.fft(c_sig,n=n_bins) \n",
    "        half_freq_domain = freqsig[:int(n_bins/2)]\n",
    "        #get index in the freq domain\n",
    "        m_freq[k] = np.sum(np.abs(half_freq_domain) * range(len(half_freq_domain)))/sum(np.abs(half_freq_domain))\n",
    "    return pd.Series(m_freq)\n",
    "\n",
    "def skewness(data, n_bins=200, filterMean=True): \n",
    "    \"\"\"skewness of the frequency domain signal\"\"\"\n",
    "    m_skew = {}\n",
    "    for k in data._get_numeric_data():\n",
    "        c_sig = []\n",
    "        if filterMean:\n",
    "            filtered = data[k].dropna().as_matrix()\n",
    "            mean_sig = np.ones_like(filtered)*np.mean(filtered)\n",
    "            # remove mean of the signal, for better results.\n",
    "            c_sig = data[k].dropna().as_matrix() - mean_sig\n",
    "        freqsig = fft.fft(c_sig,n=n_bins) \n",
    "        half_freq_domain = freqsig[:int(n_bins/2)]\n",
    "        #get index in the freq domain\n",
    "        m_skew[k] = skew(c_sig)\n",
    "    return pd.Series(m_skew)\n",
    "\n",
    "def kurtos(data, n_bins=200, filterMean=True):\n",
    "    \"\"\"kurtosis of the frequency domain signal\"\"\"\n",
    "    m_kurtosis = {}\n",
    "    for k in data._get_numeric_data():\n",
    "        c_sig = []\n",
    "        if filterMean:\n",
    "            filtered = data[k].dropna().as_matrix()\n",
    "            mean_sig = np.ones_like(filtered)*np.mean(filtered)\n",
    "            # remove mean of the signal, for better results.\n",
    "            c_sig = data[k].dropna().as_matrix() - mean_sig\n",
    "        freqsig = fft.fft(c_sig,n=n_bins) \n",
    "        half_freq_domain = freqsig[:int(n_bins/2)]\n",
    "        #get index in the freq domain\n",
    "        m_kurtosis[k] = kurtosis(c_sig)\n",
    "    return pd.Series(m_kurtosis)\n",
    "\n",
    "def bandsEnergy():\n",
    "    \"\"\"Energy of a frequency interval within the bins of the FFT.\"\"\"\n",
    "    pass\n",
    "\n",
    "def angle():\n",
    "    \"\"\"Angle between to vectors.\"\"\"\n",
    "    pass\n",
    "\n",
    "def arCoeff(): \n",
    "    \"\"\"Autorregresion coefficients with Burg order equal to 4\"\"\"\n",
    "    pass\n",
    "\n",
    "def correlation_acc(data): \n",
    "    \"\"\"correlation coefficient between two accelerometer signals\"\"\"\n",
    "    cor = data[[\"accY\", \"accX\", \"accZ\", \"gyroZ\", \"gyroX\", \"gyroY\"]].corr().to_dict()['accY']\n",
    "    res = {}\n",
    "    for k,v in cor.iteritems():\n",
    "        if k == 'accY':\n",
    "            continue\n",
    "        res[k+'-accY'] = v\n",
    "    return res\n",
    "\n",
    "def correlation_kinect(data): \n",
    "    \"\"\"correlation coefficient between two accelerometer signals\"\"\"\n",
    "    cor = flatten_dict(data[[\"ci\",\"proximity\"]].corr().to_dict()[\"ci\"])\n",
    "    res = {}\n",
    "    for k,v in cor.iteritems():\n",
    "        if k == 'ci':\n",
    "            continue\n",
    "        res[k+'-ci'] = v\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadFile(file_name, verbose=False):\n",
    "    ##NOTE: IF \"TOO MANY VALUES TO UNPACK\" ERROR IN THE getCSV METHOD, RESTART THE KERNEL. SOMETHING MUST BE WRONG WITH\n",
    "    # THE KERNEL INITIALIZATION. MUST BE CHECKED! (LOW-PRIORITY)\n",
    "\n",
    "    csv_data = None          # the variable where the loaded csv data is stored.\n",
    "    num_windows = 0          # the number of windows loaded.\n",
    "    windows = []             # the list of windows data. Each element is a pandas dataframe \n",
    "                             #  corresponding to the windows. The list is of size 'num_windows'.\n",
    "\n",
    "    print '-- Processing: \"{}\"'.format(file_name)\n",
    "\n",
    "    # load the data, abort in case of error.\n",
    "    try:\n",
    "        num_windows, csv_data = getCSV(os.path.join(csv_dir, file_name))\n",
    "    except ValueError as e:\n",
    "        print traceback.format_exc()\n",
    "        sys.exit(-1)\n",
    "\n",
    "    for w in range(num_windows):\n",
    "        win_data = {}\n",
    "        for k in csv_data.keys():\n",
    "            # consider the data only if it is not in the ignore list.\n",
    "            if k not in ignore_col_list:\n",
    "                if  csv_data[k][w] == []:\n",
    "                    raise ValueError(\"\\tFile has empty tagged windows. Skipping...\")\n",
    "                win_data[feature_name_map[k]] = csv_data[k][w]\n",
    "                \n",
    "        # convert dictionary to dataframe and save it to list of all windows data for the file.\n",
    "        windows.append(pd.DataFrame.from_dict(win_data))\n",
    "    \n",
    "    print '-- Retrieved {} windows in {}'.format(num_windows, file_name)\n",
    "    \n",
    "    if verbose:\n",
    "        overlap_reference = 50\n",
    "        try:\n",
    "            _, n_windows, sample_info, avg_overlap, avg_diff = getStatistics(csv_data, compareWith=overlap_reference)\n",
    "            print \"LOAD SUMMARY:\"\n",
    "            print tabulate([[n_windows,\"{:.2f}\".format(avg_overlap),\"{:.2f}\".format(avg_diff)]],\n",
    "                               headers=[\"#Win\", \"Avg. Overlap\", \"Avg. dev. from ref.\"])\n",
    "        except ValueError as e:\n",
    "            print traceback.format_exc()\n",
    "            return None\n",
    "    return windows\n",
    "\n",
    "def get_metadata(data, target=\"control\", all_targets=['control','expectation','high_level','activity']):\n",
    "    \"\"\"Export the metadata to a file\n",
    "    data : the list of windows data.\n",
    "    target   :   the target we want to export\n",
    "    \"\"\"\n",
    "    numeric_cols = data[0]._get_numeric_data().columns.values\n",
    "    exclude = list(set(['control','expectation','high_level','activity']) - set(['control'])) + ['time']\n",
    "    \n",
    "    X_output_filename = open(\"{}_X.csv\".format(target), 'wa')\n",
    "    y_output_filename = open(\"{}_y.csv\".format(target), 'wa')\n",
    "        \n",
    "    Xwriter = csv.DictWriter(X_output_filename, [k for k in numeric_cols if k not in exclude])\n",
    "    # write the headers\n",
    "    Xwriter.writeheader()\n",
    "    # flush data\n",
    "    X_output_filename.flush()\n",
    "    ywriter = csv.writer(y_output_filename)\n",
    "    ywriter.writerow([target])\n",
    "    for df in data:\n",
    "        rows = mean(df).to_dict()\n",
    "        del rows[\"time\"]\n",
    "        Xwriter.writerows([rows])   #write content to the file\n",
    "        #Xwriter.writerows([{}])    #write an empty line to mark the end of the windows\n",
    "        X_output_filename.flush()   #flush data.\n",
    "        ywriter.writerow([df[target][0]])\n",
    "        y_output_filename.flush()\n",
    "    \n",
    "\n",
    "def getMetadataForAll(listOfFiles, target=\"control\", all_targets=['control','expectation','high_level','activity']):\n",
    "    \"\"\"Export the metadata to a file\n",
    "    data : the list of windows data.\n",
    "    target   :   the target we want to export\n",
    "    \"\"\"\n",
    "    exclude = list(set(all_targets) - set(target)) + ['time']\n",
    "    X_output_filename = None\n",
    "    y_output_filename = None\n",
    "    Xwriter = None\n",
    "    ywriter = None\n",
    "    data = []\n",
    "    failed_files = []\n",
    "    for i, csv_filename in enumerate(listOfFiles):\n",
    "            \n",
    "        try:\n",
    "            data = loadFile(csv_filename)\n",
    "        except ValueError as e:\n",
    "            print traceback.format_exc()\n",
    "            failed_files.append(csv_filename)\n",
    "            continue\n",
    "        \n",
    "        if i == 0:\n",
    "            numeric_cols = data[0]._get_numeric_data().columns.values\n",
    "            \n",
    "            X_output_filename = open(\"{}_X.csv\".format(target), 'wa')\n",
    "            y_output_filename = open(\"{}_y.csv\".format(target), 'wa')\n",
    "\n",
    "            Xwriter = csv.DictWriter(X_output_filename, [k for k in numeric_cols if k not in exclude])\n",
    "            # write the headers\n",
    "            Xwriter.writeheader()\n",
    "            # flush data\n",
    "            X_output_filename.flush()\n",
    "            ywriter = csv.writer(y_output_filename)\n",
    "            ywriter.writerow([target])\n",
    "            \n",
    "        for df in data:\n",
    "            rows = mean(df.drop('time', 1)).to_dict()\n",
    "            Xwriter.writerows([rows])   #write content to the file\n",
    "            X_output_filename.flush()   #flush data.\n",
    "            ywriter.writerow([df[target][0]])\n",
    "            y_output_filename.flush()\n",
    "        \n",
    "        ### write an empty line to mark the end of the file\n",
    "        Xwriter.writerows([{}])\n",
    "        X_output_filename.flush()\n",
    "        ywriter.writerows([\"\"])\n",
    "        y_output_filename.flush()\n",
    "        #############################\n",
    "    print '-- List of failed files:\\n{} '.format(json.dumps(failed_files, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Processing: \"_2016-11-23-18-49-13_exp1_Player.csv\"\n",
      "-- Retrieved 24 windows in _2016-11-23-18-49-13_exp1_Player.csv\n",
      "-- Processing: \"_2016-11-23-18-49-13_exp2_Player.csv\"\n",
      "-- Retrieved 28 windows in _2016-11-23-18-49-13_exp2_Player.csv\n",
      "-- Processing: \"_2016-11-23-18-49-13_exp3_Player.csv\"\n",
      "-- Retrieved 12 windows in _2016-11-23-18-49-13_exp3_Player.csv\n",
      "-- Processing: \"_2016-11-23-18-49-13_exp4_Player.csv\"\n",
      "-- Retrieved 23 windows in _2016-11-23-18-49-13_exp4_Player.csv\n",
      "-- Processing: \"_2016-11-23-18-49-13_exp5_Player.csv\"\n",
      "-- Retrieved 27 windows in _2016-11-23-18-49-13_exp5_Player.csv\n",
      "-- Processing: \"_2016-11-24-15-43-37_exp1d_Player.csv\"\n",
      "-- Retrieved 31 windows in _2016-11-24-15-43-37_exp1d_Player.csv\n",
      "-- Processing: \"_2016-11-24-15-43-37_exp2d_Player.csv\"\n",
      "-- Retrieved 48 windows in _2016-11-24-15-43-37_exp2d_Player.csv\n",
      "-- Processing: \"_2016-11-24-15-43-37_exp3d_Player.csv\"\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-30-70d5bdb09872>\", line 88, in getMetadataForAll\n",
      "    data = loadFile(csv_filename)\n",
      "  File \"<ipython-input-30-70d5bdb09872>\", line 25, in loadFile\n",
      "    raise ValueError(\"\\tFile has empty tagged windows. Skipping...\")\n",
      "ValueError: \tFile has empty tagged windows. Skipping...\n",
      "\n",
      "-- Processing: \"_2016-11-24-15-43-37_exp4d_Player.csv\"\n",
      "-- Retrieved 18 windows in _2016-11-24-15-43-37_exp4d_Player.csv\n",
      "-- Processing: \"_2016-11-24-15-43-37_exp5d_Player.csv\"\n",
      "-- Retrieved 18 windows in _2016-11-24-15-43-37_exp5d_Player.csv\n",
      "-- Processing: \"_2016-11-24-15-43-37_exp6d_Player.csv\"\n",
      "-- Retrieved 30 windows in _2016-11-24-15-43-37_exp6d_Player.csv\n",
      "-- Processing: \"_2016-11-24-16-23-29_expa_Player.csv\"\n",
      "-- Retrieved 16 windows in _2016-11-24-16-23-29_expa_Player.csv\n",
      "-- Processing: \"_2016-11-24-16-23-29_expb_Player.csv\"\n",
      "-- Retrieved 26 windows in _2016-11-24-16-23-29_expb_Player.csv\n",
      "-- Processing: \"_2016-11-24-16-23-29_expc_Player.csv\"\n",
      "-- Retrieved 32 windows in _2016-11-24-16-23-29_expc_Player.csv\n",
      "-- Processing: \"_2016-11-24-16-23-29_expd_Player.csv\"\n",
      "-- Retrieved 29 windows in _2016-11-24-16-23-29_expd_Player.csv\n",
      "-- Processing: \"_2016-11-24-16-48-48_exp1d_Player.csv\"\n",
      "-- Retrieved 46 windows in _2016-11-24-16-48-48_exp1d_Player.csv\n",
      "-- Processing: \"_2016-11-24-16-48-48_exp2d_Player.csv\"\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-30-70d5bdb09872>\", line 88, in getMetadataForAll\n",
      "    data = loadFile(csv_filename)\n",
      "  File \"<ipython-input-30-70d5bdb09872>\", line 25, in loadFile\n",
      "    raise ValueError(\"\\tFile has empty tagged windows. Skipping...\")\n",
      "ValueError: \tFile has empty tagged windows. Skipping...\n",
      "\n",
      "-- Processing: \"_2016-11-24-16-48-48_exp3d_Player.csv\"\n",
      "-- Retrieved 24 windows in _2016-11-24-16-48-48_exp3d_Player.csv\n",
      "-- Processing: \"_2016-11-24-17-15-38_expa_Player.csv\"\n",
      "-- Retrieved 22 windows in _2016-11-24-17-15-38_expa_Player.csv\n",
      "-- Processing: \"_2016-11-24-17-15-38_expb_Player.csv\"\n",
      "-- Retrieved 15 windows in _2016-11-24-17-15-38_expb_Player.csv\n",
      "-- Processing: \"_2016-11-24-17-15-38_expc_Player.csv\"\n",
      "-- Retrieved 18 windows in _2016-11-24-17-15-38_expc_Player.csv\n",
      "-- Processing: \"_2016-11-24-17-40-06_expb_Player.csv\"\n",
      "-- Retrieved 54 windows in _2016-11-24-17-40-06_expb_Player.csv\n",
      "-- Processing: \"_2016-11-26-15-42-51_exp1d_Player.csv\"\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-30-70d5bdb09872>\", line 88, in getMetadataForAll\n",
      "    data = loadFile(csv_filename)\n",
      "  File \"<ipython-input-30-70d5bdb09872>\", line 25, in loadFile\n",
      "    raise ValueError(\"\\tFile has empty tagged windows. Skipping...\")\n",
      "ValueError: \tFile has empty tagged windows. Skipping...\n",
      "\n",
      "-- Processing: \"_2016-11-26-16-05-47_exp1d_Player.csv\"\n",
      "-- Retrieved 46 windows in _2016-11-26-16-05-47_exp1d_Player.csv\n",
      "-- Processing: \"_2016-11-26-16-35-21_exp1d_Player.csv\"\n",
      "-- Retrieved 61 windows in _2016-11-26-16-35-21_exp1d_Player.csv\n",
      "-- Processing: \"_2016-11-26-16-49-44_exp1d_Player.csv\"\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-30-70d5bdb09872>\", line 88, in getMetadataForAll\n",
      "    data = loadFile(csv_filename)\n",
      "  File \"<ipython-input-30-70d5bdb09872>\", line 25, in loadFile\n",
      "    raise ValueError(\"\\tFile has empty tagged windows. Skipping...\")\n",
      "ValueError: \tFile has empty tagged windows. Skipping...\n",
      "\n",
      "-- Processing: \"_2016-11-26-17-15-53_exp2_Player.csv\"\n",
      "-- Retrieved 32 windows in _2016-11-26-17-15-53_exp2_Player.csv\n",
      "-- Processing: \"_2016-11-26-17-15-53_exp3_Player.csv\"\n",
      "-- Retrieved 28 windows in _2016-11-26-17-15-53_exp3_Player.csv\n",
      "-- Processing: \"_2016-11-26-17-15-53_exp4_Player.csv\"\n",
      "-- Retrieved 32 windows in _2016-11-26-17-15-53_exp4_Player.csv\n",
      "-- Processing: \"_2016-11-26-17-15-53_fixed_exp1d_Player.csv\"\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-30-70d5bdb09872>\", line 88, in getMetadataForAll\n",
      "    data = loadFile(csv_filename)\n",
      "  File \"<ipython-input-30-70d5bdb09872>\", line 25, in loadFile\n",
      "    raise ValueError(\"\\tFile has empty tagged windows. Skipping...\")\n",
      "ValueError: \tFile has empty tagged windows. Skipping...\n",
      "\n",
      "-- Processing: \"_2016-11-26-17-15-53_fixed_exp2d_Player.csv\"\n",
      "-- Retrieved 25 windows in _2016-11-26-17-15-53_fixed_exp2d_Player.csv\n",
      "-- Processing: \"_2016-11-26-17-38-21_fixed_exp1d_Player.csv\"\n",
      "-- Retrieved 45 windows in _2016-11-26-17-38-21_fixed_exp1d_Player.csv\n",
      "-- Processing: \"_2016-11-26-18-36-15_expa_Player.csv\"\n",
      "-- Retrieved 19 windows in _2016-11-26-18-36-15_expa_Player.csv\n",
      "-- Processing: \"_2016-11-26-18-36-15_expb_Player.csv\"\n",
      "-- Retrieved 72 windows in _2016-11-26-18-36-15_expb_Player.csv\n",
      "-- List of failed files:\n",
      "[\n",
      "    \"_2016-11-24-15-43-37_exp3d_Player.csv\", \n",
      "    \"_2016-11-24-16-48-48_exp2d_Player.csv\", \n",
      "    \"_2016-11-26-15-42-51_exp1d_Player.csv\", \n",
      "    \"_2016-11-26-16-49-44_exp1d_Player.csv\", \n",
      "    \"_2016-11-26-17-15-53_fixed_exp1d_Player.csv\"\n",
      "] \n"
     ]
    }
   ],
   "source": [
    "getMetadataForAll(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
