{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### \n",
    "# Created by Ewerton Lopes\n",
    "# Politecnico di Milano, December, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import traceback\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from pandas.tools.plotting import lag_plot, autocorrelation_plot\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.stats import norm, skew, kurtosis\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "import scipy.fftpack as fft\n",
    "\n",
    "from helper_functions import getListOfFiles, getCSV, getStatistics, remap_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:** </br>\n",
    "1. Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features selected for this dataset come from the accelerometer and gyroscope 3-axial raw signals tAcc-XYZ and tGyro-XYZ. That signal was taken using the <a href=http://playground.arduino.cc/Main/MPU-6050> InvenSense MPU-6050 </a> sensor, which contains a MEMS accelerometer and a MEMS gyro in a single chip. The sensor possess a 16-bits analog to digital conversion hardware for each channel. Therefor it captures the x, y, and z channel at the same time. The sensor was used with the I2C-bus to interface with an Arduino attached to the player.  We also use a Microsoft Kinect 2 sensor in order to get 3 additional features: Contraction Index (CI), distance, proximity. This last feature is a normalized - [0,1] interval - version of distance. Thus, they shall be used in mutually exclusion. The normalization is done by limiting the distance to 4.5 meters, the known stable limit of the sensor. So, the player has a greater proximity value when it is seen close to the robot. The value tends to zero otherwise. Ideally, the CI feature correspond to how open-wide (in terms of legs and arms) the player is.\n",
    "\n",
    "Here, we organize the feature similar to <a href=https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones> Anguita at al.</a>. The time domain signals are (prefix 't' to denote time) were captured at a about rate of 48 Hz for the MPU-6050 based data. <font color='red'>Then they were filtered using a median filter and a 3rd order low pass Butterworth filter with a corner frequency of 20 Hz to remove noise. Similarly, the acceleration signal was then separated into body and gravity acceleration signals (tBodyAcc-XYZ and tGravityAcc-XYZ) using another low pass Butterworth filter with a corner frequency of 0.3 Hz.\n",
    "\n",
    "Subsequently, the body linear acceleration and angular velocity were derived in time to obtain Jerk signals\n",
    "(tBodyAccJerk-XYZ and tBodyGyroJerk-XYZ). Also the magnitude of these three-dimensional signals were calculated\n",
    "using the Euclidean norm (tBodyAccMag, tGravityAccMag, tBodyAccJerkMag, tBodyGyroMag, tBodyGyroJerkMag).\n",
    "\n",
    "Finally a Fast Fourier Transform (FFT) was applied to some of these signals producing fBodyAcc-XYZ,\n",
    "fBodyAccJerk-XYZ, fBodyGyro-XYZ, fBodyAccJerkMag, fBodyGyroMag, fBodyGyroJerkMag.\n",
    "(Note the 'f' to indicate frequency domain signals).\n",
    "\n",
    "These signals were used to estimate variables of the feature vector for each pattern:  \n",
    "'-XYZ' is used to denote 3-axial signals in the X, Y and Z directions.\n",
    "\n",
    "tBodyAcc-XYZ\n",
    "tGravityAcc-XYZ\n",
    "tBodyAccJerk-XYZ\n",
    "tBodyGyro-XYZ\n",
    "tBodyGyroJerk-XYZ\n",
    "tBodyAccMag\n",
    "tGravityAccMag\n",
    "tBodyAccJerkMag\n",
    "tBodyGyroMag\n",
    "tBodyGyroJerkMag\n",
    "fBodyAcc-XYZ\n",
    "fBodyAccJerk-XYZ\n",
    "fBodyGyro-XYZ\n",
    "fBodyAccMag\n",
    "fBodyAccJerkMag\n",
    "fBodyGyroMag\n",
    "fBodyGyroJerkMag\n",
    "\n",
    "The set of variables that were estimated from these signals are: \n",
    "\n",
    "mean(): Mean value\n",
    "std(): Standard deviation\n",
    "mad(): Median absolute deviation \n",
    "max(): Largest value in array\n",
    "min(): Smallest value in array\n",
    "sma(): Signal magnitude area\n",
    "energy(): Energy measure. Sum of the squares divided by the number of values. \n",
    "iqr(): Interquartile range \n",
    "entropy(): Signal entropy\n",
    "arCoeff(): Autorregresion coefficients with Burg order equal to 4\n",
    "correlation(): correlation coefficient between two signals\n",
    "maxInds(): index of the frequency component with largest magnitude\n",
    "meanFreq(): Weighted average of the frequency components to obtain a mean frequency\n",
    "skewness(): skewness of the frequency domain signal \n",
    "kurtosis(): kurtosis of the frequency domain signal \n",
    "bandsEnergy(): Energy of a frequency interval within the 64 bins of the FFT of each window.\n",
    "angle(): Angle between to vectors.\n",
    "\n",
    "Additional vectors obtained by averaging the signals in a signal window sample. These are used on the angle() variable:\n",
    "\n",
    "gravityMean\n",
    "tBodyAccMean\n",
    "tBodyAccJerkMean\n",
    "tBodyGyroMean\n",
    "tBodyGyroJerkMean\n",
    "\n",
    "The complete list of variables of each feature vector is available in 'features.txt' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the csv files, that together comprise the our dataset, from the `.data` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 34 CSV Files found:\n",
      "\n",
      "[\"_2016-11-23-18-49-13_exp1_Player.csv\", \"_2016-11-23-18-49-13_exp2_Player.csv\", \"_2016-11-23-18-49-13_exp3_Player.csv\", \"_2016-11-23-18-49-13_exp4_Player.csv\", \"_2016-11-23-18-49-13_exp5_Player.csv\", \"_2016-11-24-15-43-37_exp1d_Player.csv\", \"_2016-11-24-15-43-37_exp2d_Player.csv\", \"_2016-11-24-15-43-37_exp3d_Player.csv\", \"_2016-11-24-15-43-37_exp4d_Player.csv\", \"_2016-11-24-15-43-37_exp5d_Player.csv\", \"_2016-11-24-15-43-37_exp6d_Player.csv\", \"_2016-11-24-16-23-29_expa_Player.csv\", \"_2016-11-24-16-23-29_expb_Player.csv\", \"_2016-11-24-16-23-29_expc_Player.csv\", \"_2016-11-24-16-23-29_expd_Player.csv\", \"_2016-11-24-16-48-48_exp1d_Player.csv\", \"_2016-11-24-16-48-48_exp2d_Player.csv\", \"_2016-11-24-16-48-48_exp3d_Player.csv\", \"_2016-11-24-17-15-38_expa_Player.csv\", \"_2016-11-24-17-15-38_expb_Player.csv\", \"_2016-11-24-17-15-38_expc_Player.csv\", \"_2016-11-24-17-40-06_expb_Player.csv\", \"_2016-11-26-15-42-51_exp1d_Player.csv\", \"_2016-11-26-16-05-47_exp1d_Player.csv\", \"_2016-11-26-16-35-21_exp1d_Player.csv\", \"_2016-11-26-16-49-44_exp1d_Player.csv\", \"_2016-11-26-17-15-53_exp2_Player.csv\", \"_2016-11-26-17-15-53_exp3_Player.csv\", \"_2016-11-26-17-15-53_exp4_Player.csv\", \"_2016-11-26-17-15-53_fixed_exp1d_Player.csv\", \"_2016-11-26-17-15-53_fixed_exp2d_Player.csv\", \"_2016-11-26-17-38-21_fixed_exp1d_Player.csv\", \"_2016-11-26-18-36-15_expa_Player.csv\", \"_2016-11-26-18-36-15_expb_Player.csv\"]\n"
     ]
    }
   ],
   "source": [
    "csv_dir = \"../data/annotated_csv\"\n",
    "files = getListOfFiles(csv_dir, \".csv\")\n",
    "print \">> {} CSV Files found:\\n\".format(len(files))\n",
    "print json.dumps(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************\n",
      "                         OVERALL SUMMARY\n",
      "*****************************************************************\n",
      "\n",
      "CSV dir: \t../data/annotated_csv\n",
      "N.Files: \t34\n",
      "Ref. overlap: \t50%\n",
      "N. Windows (Avg): \t31.07\n",
      "Overlap    (Avg): \t49.99%\n",
      "Overlap    (Std) : \t0.04%\n",
      "Overpap dev. from ref. (Avg): \t-0.01%\n",
      "Overpap dev. from ref. (Std): \t0.04%\n",
      "\n",
      "Topic                              Avg #Samples per Windows    Std\n",
      "-------------------------------  --------------------------  -----\n",
      "Control                                              182.72  24.06\n",
      "robogame/imu_state.linear_acc.x                      138.18  24.51\n",
      "robogame/imu_state.linear_acc.z                      138.18  24.51\n",
      "robogame/imu_state.gyro.z                            138.18  24.51\n",
      "robogame/imu_state.gyro.x                            138.18  24.51\n",
      "robogame/imu_state.linear_acc.y                      138.18  24.51\n",
      "robogame/imu_state.gyro.y                            138.18  24.51\n",
      "Expectation                                          182.72  24.06\n",
      "High_level                                           182.72  24.06\n",
      "Activity                                             182.72  24.06\n",
      "/kinect_features/.ci                                  44.62   2.26\n",
      "/kinect_features/.distance                            44.62   2.26\n",
      "/kinect_features/.proximity                           44.62   2.26\n"
     ]
    }
   ],
   "source": [
    "# Get details about the data stored on the csv files.\n",
    "%run data_summary.py -f ../data/annotated_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pdf(data,suptitle=\"PDF fitting\", headers=\"\", normalize_data=False):\n",
    "    \"\"\"Plot the probability density function.\n",
    "    Args:\n",
    "        data (list: pd.Series): the list of data to be used.\n",
    "        headers (list: str)   : list of titles for the plots\n",
    "        normalize_data : normalize data prior to fit the pdf.\n",
    "    \"\"\"\n",
    "\n",
    "    grid_side_size = int(round(np.sqrt(len(data))))\n",
    "    #fig = plt.figure(figsize=(11,7))\n",
    "    fig, axes = plt.subplots(grid_side_size, grid_side_size, figsize=(18,12))\n",
    "    \n",
    "    count = 0\n",
    "    for i, row in enumerate(axes):\n",
    "        for j in range(grid_side_size):\n",
    "            if count >= len(data):\n",
    "                break\n",
    "                \n",
    "            #normalize vector\n",
    "            if normalize_data:\n",
    "                n_data = data[count]/data[count].sum()\n",
    "            else:\n",
    "                n_data = data[count]\n",
    "                \n",
    "            # Fit a normal distribution to the data:\n",
    "            mu, sigma = norm.fit(n_data)\n",
    "            \n",
    "            if headers == \"\":\n",
    "                row[j].set_title(\"{}-Fit. mu={:.2f},  std={:.2f}\".format(count, mu, sigma), fontsize=8)\n",
    "            else:\n",
    "                row[j].set_title(\"Fit for {}. mu={:.2f},  std={:.2f}\".format(headers[count], mu, sigma), fontsize=8)\n",
    "            \n",
    "            # Using the  Freedman-Diaconis rule for getting the number of bins\n",
    "            bin_size = 2 * iqr(n_data) * len(n_data) ** (-1 / 3)\n",
    "            bins = int(round((max(n_data) - min(n_data)) / bin_size))\n",
    "\n",
    "            # Plot the histogram.\n",
    "            bin_value, bin_edges, p = row[j].hist(n_data, bins=bins, normed=True, alpha=0.6, color='g')\n",
    "            \n",
    "            # Normalizing heights\n",
    "            #debug = 0\n",
    "            #for i, item in enumerate(p):\n",
    "            #    debug += (bin_value[i] / sum(bin_value*np.diff(bin_edges)))\n",
    "            #    item.set = bin_value[i]/ sum(bin_value*np.diff(bin_edges))\n",
    "            #print debug\n",
    "\n",
    "            # Plot the PDF.\n",
    "            xmin, xmax = row[j].get_xlim()\n",
    "            x = np.linspace(xmin, xmax, 100)\n",
    "            p = norm.pdf(x, mu, sigma)\n",
    "            row[j].plot(x, p, 'k', linewidth=2, color='r')\n",
    "            count += 1\n",
    "\n",
    "    fig.suptitle(suptitle, fontsize=21)\n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "    plt.draw()\n",
    "\n",
    "def gridOfPlots(data, columnToPlot, feature='activity', suptitle=\"Grid of plot\"):\n",
    "    \"\"\"Plots the data in a grid of plots.\n",
    "    Args:\n",
    "        data (list): the list of data to be used.\n",
    "        title (str): grid title.\n",
    "        columnToPlot: the column in the data to be plotted.\n",
    "    \"\"\"\n",
    "    \n",
    "    grid_side_size = int(round(np.sqrt(len(data))))\n",
    "    fig, axes = plt.subplots(grid_side_size, grid_side_size, figsize=(18,12))\n",
    "\n",
    "    count = 0\n",
    "    for i, row in enumerate(axes):\n",
    "        for j in range(grid_side_size):\n",
    "            if count >= len(data):\n",
    "                fig.delaxes(row[j])\n",
    "            else:\n",
    "                #norm_data = [remap_interval(x,-32768,32768,0,1) for x in data[count][columnToPlot].dropna()]\n",
    "                ## According to the MPU-6050 datasheet, page 13, you can convert the raw accelerometer\n",
    "                # data into multiples of g (9.8 m/s^2) by dividing by a factor of 16384. \n",
    "                norm_data = [x/16384.0 for x in data[count][columnToPlot].dropna()]\n",
    "                row[j].set_title(\"W={}. {}={} \".format(count,feature, data[count][feature][0]), fontsize=8, fontweight=\"bold\")\n",
    "                row[j].set_ylabel('g\\'s (9.8 m/s^2)')\n",
    "                row[j].plot(norm_data, label=str(data[count][\"high_level\"][0]))\n",
    "                row[j].grid()\n",
    "                count += 1\n",
    "\n",
    "    fig.suptitle(suptitle, fontsize=21)\n",
    "    fig.subplots_adjust(hspace=0.5, wspace= 0.4)\n",
    "    plt.draw()\n",
    "\n",
    "def gridOfLagPlots(data, columnToPlot, suptitle=\"Plot\"):\n",
    "    \"\"\"Plots a grid of plots corresponding to a lag-one time series \n",
    "    graph for the given column.\n",
    "    \n",
    "    The 'abscissa' correspond to the column value at time t. The 'ordinate' correspond\n",
    "    to the column value at time t-1. Hence the name 'lag-one' time series plot.\n",
    "    \n",
    "    Args:\n",
    "        data (list): the list of data to be used.\n",
    "        title (str): grid title.\n",
    "        columnToPlot: the column in the data to be plotted.\n",
    "    \"\"\"\n",
    "    grid_side_size = int(round(np.sqrt(len(data))))\n",
    "    plt.figure(figsize=(18,12))\n",
    "\n",
    "    count = 0\n",
    "    for i in range(grid_side_size):\n",
    "        for j in range(grid_side_size):\n",
    "            if count >= len(data):\n",
    "                break\n",
    "            plt.subplot(grid_side_size,grid_side_size,count+1)\n",
    "            lag_plot(data[count][columnToPlot].dropna())\n",
    "            plt.title(\"Windows-{}\".format(count))\n",
    "            count += 1\n",
    "\n",
    "\n",
    "    plt.suptitle(suptitle, fontsize=21)\n",
    "    plt.subplots_adjust(hspace=0.5, wspace= 0.4)\n",
    "    plt.draw()\n",
    "    \n",
    "def plotAcc(data, suptitle=\"Accelerometer signal\", feature= 'High_level', plt_win_func=False):\n",
    "    \"\"\"Plots the data in a grid of plots.\n",
    "    Args:\n",
    "        data (list): the list of data to be used.\n",
    "        title (str): grid title.\n",
    "        columnToPlot: the column in the data to be plotted.\n",
    "    \"\"\"\n",
    "    \n",
    "    grid_side_size = int(round(np.sqrt(len(data))))\n",
    "    fig, axes = plt.subplots(grid_side_size, grid_side_size, figsize=(18,12))\n",
    "\n",
    "    count = 0\n",
    "    for i, row in enumerate(axes):\n",
    "        for j in range(grid_side_size):\n",
    "            if count >= len(data):\n",
    "                fig.delaxes(row[j])\n",
    "            else:\n",
    "                ## According to the MPU-6050 datasheet, page 13, you can convert the raw accelerometer\n",
    "                # data into multiples of g (9.8 m/s^2) by dividing by a factor of 16384. \n",
    "                norm_dataX = [x/16384.0 for x in data[count][\"accX\"].dropna()]\n",
    "                norm_dataY = [y/16384.0 for y in data[count][\"accY\"].dropna()]\n",
    "                norm_dataZ = [z/16384.0 for z in data[count][\"accZ\"].dropna()]\n",
    "\n",
    "                row[j].set_title(\"W={}. {}={} \".format(count, feature, data[count][feature][0]), fontsize=8, fontweight=\"bold\")\n",
    "                row[j].set_ylabel('g\\'s (9.8 m/s^2)')\n",
    "                row[j].set_ylim((-1,1))\n",
    "                row[j].set_xlim((0,len(norm_dataX)))\n",
    "                row[j].plot(norm_dataX, label=\"x\")\n",
    "                row[j].plot(norm_dataY, label=\"y\")\n",
    "                row[j].plot(norm_dataZ, label=\"z\")\n",
    "                # draw a default vline at x=1 that spans the yrange\n",
    "                row[j].axvline(x=len(norm_dataX)/2, color='g', marker='.')\n",
    "                row[j].grid()\n",
    "                count += 1\n",
    "                row[j].legend(loc='upper center', bbox_to_anchor=(0.5, 1.45), ncol=3, fontsize=8, fancybox=True, shadow=True)\n",
    "    \n",
    "    fig.suptitle(suptitle, fontsize=21)\n",
    "    fig.subplots_adjust(hspace=0.7, wspace= 0.5)\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a dictionary containing a remap of the name of the features. This is to easy visualization since ros feature names may be very long and also allow quick decoupled modifications (just edit the variable locally). Also, from it we define a ignore list containing the names we do not want to consider for the analysis. That is done by setting to the ignore list all dictionary keys with empty values. **Here we assume the names are consistent feature name w.r.t. the `csv` data. Otherwise, a `ValueError` exception is likely to be thrown!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of ignored topics: []\n"
     ]
    }
   ],
   "source": [
    "# variable for storing the loaded feature names.\n",
    "feature_name_map = {\n",
    "  \"time\" : \"time\",\n",
    "  \"Control\": \"control\",\n",
    "  \"High_level\": \"high_level\",\n",
    "  \"Expectation\": \"expectation\",\n",
    "  \"Activity\": \"activity\",\n",
    "  \"/kinect_features/.ci\": \"ci\",\n",
    "  \"/kinect_features/.distance\": \"distance\",\n",
    "  \"/kinect_features/.proximity\": \"proximity\",\n",
    "  \"robogame/imu_state.gyro.x\": \"gyroX\",\n",
    "  \"robogame/imu_state.gyro.y\": \"gyroY\",\n",
    "  \"robogame/imu_state.gyro.z\": \"gyroZ\",\n",
    "  \"robogame/imu_state.linear_acc.x\": \"accX\",\n",
    "  \"robogame/imu_state.linear_acc.y\": \"accY\",\n",
    "  \"robogame/imu_state.linear_acc.z\": \"accZ\"\n",
    "}\n",
    "\n",
    "ignore_col_list = [k for k,v in feature_name_map.items() if v is \"\"]\n",
    "print \"List of ignored topics: {}\".format(ignore_col_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin the analysis using only one file. Of course, once it is completed, we extend the analysis for the other files in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from helper_functions import getListOfFiles, getCSV, getStatistics, remap_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: \"_2016-11-23-18-49-13_exp2_Player.csv\"\n",
      "Retrieved 28 windows in _2016-11-23-18-49-13_exp2_Player.csv\n",
      "LOAD SUMMARY:\n",
      "  #Win    Avg. Overlap    Avg. dev. from ref.\n",
      "------  --------------  ---------------------\n",
      "    28           49.97                  -0.03\n"
     ]
    }
   ],
   "source": [
    "##NOTE: IF \"TOO MANY VALUES TO UNPACK\" ERROR IN THE getCSV METHOD, RESTART THE KERNEL. SOMETHING MUST BE WRONG WITH\n",
    "# THE KERNEL INITIALIZATION. MUST BE CHECKED! (LOW-PRIORITY)\n",
    "\n",
    "csv_filename = files[1]  # get only the fist loaded csv file.\n",
    "csv_data = None          # the variable where the loaded csv data is stored.\n",
    "num_windows = 0          # the number of windows loaded.\n",
    "wFrames = []             # the list of windows data. Each element is a pandas dataframe \n",
    "                         #  corresponding to the windows. The list is of size 'num_windows'.\n",
    "\n",
    "print 'Processing: \"{}\"'.format(csv_filename)\n",
    "\n",
    "# load the data, abort in case of error.\n",
    "try:\n",
    "    num_windows, csv_data = getCSV(os.path.join(csv_dir, csv_filename))\n",
    "except ValueError as e:\n",
    "    print traceback.format_exc()\n",
    "    sys.exit(-1)\n",
    "\n",
    "for w in range(num_windows):\n",
    "    win_data = {}\n",
    "    for k in csv_data.keys():\n",
    "        # consider the data only if it is not in the ignore list.\n",
    "        if k not in ignore_col_list:\n",
    "            win_data[feature_name_map[k]] = csv_data[k][w]\n",
    "\n",
    "    # convert dictionary to dataframe and save it to list of all windows data for the file.\n",
    "    wFrames.append(pd.DataFrame.from_dict(win_data))\n",
    "print 'Retrieved {} windows in {}'.format(num_windows, csv_filename)\n",
    "overlap_reference = 50\n",
    "_, n_windows, sample_info, avg_overlap, avg_diff = getStatistics(csv_data, compareWith=overlap_reference)\n",
    "\n",
    "print \"LOAD SUMMARY:\"\n",
    "print tabulate([[n_windows,\"{:.2f}\".format(avg_overlap),\"{:.2f}\".format(avg_diff)]],\n",
    "                   headers=[\"#Win\", \"Avg. Overlap\", \"Avg. dev. from ref.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the accelerometer signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accmag(data, suptitle=\"Magnitude of accelerometer signal\", feature='High_level', plt_win_func=False):\n",
    "    \"\"\"Plots the data in a grid of plots.\n",
    "    Args:\n",
    "        data (list): the list of data to be used.\n",
    "        title (str): grid title.\n",
    "        columnToPlot: the column in the data to be plotted.\n",
    "    \"\"\"\n",
    "    \n",
    "    grid_side_size = int(round(np.sqrt(len(data))))\n",
    "    fig, axes = plt.subplots(grid_side_size, grid_side_size, figsize=(18,12))\n",
    "\n",
    "    count = 0\n",
    "    for i, row in enumerate(axes):\n",
    "        for j in range(grid_side_size):\n",
    "            if count >= len(data):\n",
    "                fig.delaxes(row[j])\n",
    "            else:\n",
    "                ## According to the MPU-6050 datasheet, page 13, you can convert the raw accelerometer\n",
    "                # data into multiples of g (9.8 m/s^2) by dividing by a factor of 16384. \n",
    "                norm_dataX = [x/16384.0 for x in data[count][\"accX\"].dropna()]\n",
    "                norm_dataY = [y/16384.0 for y in data[count][\"accY\"].dropna()]\n",
    "                norm_dataZ = [z/16384.0 for z in data[count][\"accZ\"].dropna()]\n",
    "\n",
    "                mag = []\n",
    "                for i in range(len(norm_dataX)):\n",
    "                    mag.append(np.sqrt(norm_dataX[i]**2 + norm_dataY[i]**2 + norm_dataZ[i]**2))\n",
    "\n",
    "                windowFunction = np.hanning(len(mag))\n",
    "                #if use_win_func:\n",
    "                #    mag = mag * np.hanning(len(mag))\n",
    "\n",
    "                row[j].set_title(\"W={}. {}={} \".format(count, feature, data[count][feature][0]), fontsize=8, fontweight=\"bold\")\n",
    "                row[j].set_ylabel('Vector magnitude')\n",
    "                row[j].plot(mag, label=str(data[count][feature][0]))\n",
    "                row[j].plot(windowFunction, label=\"hanning\")\n",
    "                row[j].grid()\n",
    "                if plt_win_func:\n",
    "                    row[j].plot(mag * np.hanning(len(mag)), label=\"combined\")\n",
    "                count += 1\n",
    "                row[j].legend(loc='upper center', bbox_to_anchor=(0.5, 1.45), ncol=3, fontsize=8, fancybox=True, shadow=True)\n",
    "    \n",
    "    fig.suptitle(suptitle, fontsize=21)\n",
    "    fig.subplots_adjust(hspace=0.7, wspace= 0.5)\n",
    "    plt.draw()\n",
    "    \n",
    "    \n",
    "def running_mean(l, N):\n",
    "    # Also works for the(strictly invalid) cases when N is even.\n",
    "    if (N//2)*2 == N:\n",
    "        N = N - 1\n",
    "    front = np.zeros(N//2)\n",
    "    back = np.zeros(N//2)\n",
    "\n",
    "    for i in range(1, (N//2)*2, 2):\n",
    "        front[i//2] = np.convolve(l[:i], np.ones((i,))/i, mode = 'valid')\n",
    "    for i in range(1, (N//2)*2, 2):\n",
    "        back[i//2] = np.convolve(l[-i:], np.ones((i,))/i, mode = 'valid')\n",
    "    return np.concatenate([front, np.convolve(l, np.ones((N,))/N, mode = 'valid'), back[::-1]])\n",
    "\n",
    "def compute_runavg_acc(data, suptitle=\"Running average smoothing\", feature='activity', N=4):\n",
    "    \"\"\"Running average as smoothing\"\"\"\n",
    "    grid_side_size = int(round(np.sqrt(len(data))))\n",
    "    fig, axes = plt.subplots(grid_side_size, grid_side_size, figsize=(18,12))\n",
    "\n",
    "    count = 0\n",
    "    for i, row in enumerate(axes):\n",
    "        for j in range(grid_side_size):\n",
    "            if count >= len(data):\n",
    "                fig.delaxes(row[j])\n",
    "            else:\n",
    "                ## According to the MPU-6050 datasheet, page 13, you can convert the raw accelerometer\n",
    "                # data into multiples of g (9.8 m/s^2) by dividing by a factor of 16384. \n",
    "                norm_dataX = [x/16384.0 for x in data[count][\"accX\"].dropna()]\n",
    "                norm_dataX = running_mean(norm_dataX, N)\n",
    "                norm_dataY = [y/16384.0 for y in data[count][\"accY\"].dropna()]\n",
    "                norm_dataY = running_mean(norm_dataY, N)\n",
    "                norm_dataZ = [z/16384.0 for z in data[count][\"accZ\"].dropna()]\n",
    "                norm_dataZ = running_mean(norm_dataZ, N)\n",
    "\n",
    "                row[j].set_title(\"W={}. {}={} \".format(count, feature, data[count][feature][0]), fontsize=8, fontweight=\"bold\")\n",
    "                row[j].set_ylabel('g\\'s (9.8 m/s^2)')\n",
    "                row[j].plot(norm_dataX, label=\"x\")\n",
    "                row[j].plot(norm_dataY, label=\"y\")\n",
    "                row[j].plot(norm_dataZ, label=\"z\")\n",
    "                row[j].grid()\n",
    "                count += 1\n",
    "                row[j].legend(loc='upper center', bbox_to_anchor=(0.5, 1.45), ncol=3, fontsize=8, fancybox=True, shadow=True)\n",
    "                \n",
    "    fig.suptitle(suptitle + \" -- Order=\" + str(N), fontsize=21)\n",
    "    fig.subplots_adjust(hspace=0.7, wspace= 0.5)\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotAcc(wFrames)\n",
    "#accmag(wFrames, plt_win_func=True)\n",
    "#compute_runavg_acc(wFrames, N=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Time Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance_cost_plot(distances):\n",
    "    im = plt.imshow(distances, interpolation='nearest', cmap='Reds') \n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.grid()\n",
    "    plt.colorbar()\n",
    "\n",
    "def path_cost(x, y, accumulated_cost, distances):\n",
    "    path = [[len(x)-1, len(y)-1]]\n",
    "    cost = 0\n",
    "    i = len(y)-1\n",
    "    j = len(x)-1\n",
    "    while i>0 and j>0:\n",
    "        if i==0:\n",
    "            j = j - 1\n",
    "        elif j==0:\n",
    "            i = i - 1\n",
    "        else:\n",
    "            if accumulated_cost[i-1, j] == min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]):\n",
    "                i = i - 1\n",
    "            elif accumulated_cost[i, j-1] == min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]):\n",
    "                j = j-1\n",
    "            else:\n",
    "                i = i - 1\n",
    "                j= j- 1\n",
    "        path.append([j, i])\n",
    "    path.append([0,0])\n",
    "    for [y, x] in path:\n",
    "        cost = cost +distances[x, y]\n",
    "    return path, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = np.linspace(0, 6.28, 100)\n",
    "#x = [s/16384.0 for s in wFrames[7][\"accX\"].dropna()]\n",
    "#y = [s/16384.0 for s in wFrames[7][\"accX\"].dropna()]\n",
    "loaded = [s/16384.0 for s in wFrames[11][\"accX\"].dropna()]\n",
    "x = loaded[:int(len(loaded)/2)]\n",
    "y = loaded[int(len(loaded)/2):]\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(1,3,1)\n",
    "\n",
    "distances = np.zeros((len(y), len(x)))\n",
    "for i in range(len(y)):\n",
    "    for j in range(len(x)):\n",
    "        distances[i,j] = (x[j]-y[i])**2\n",
    "\n",
    "distance_cost_plot(distances)\n",
    "\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "accumulated_cost = np.zeros((len(y), len(x)))\n",
    "accumulated_cost[0,0] = distances[0,0]\n",
    "\n",
    "for i in range(1, len(y)):\n",
    "    accumulated_cost[i,0] = distances[i, 0] + accumulated_cost[i-1, 0]\n",
    "for i in range(1, len(x)):\n",
    "    accumulated_cost[0,i] = distances[0,i] + accumulated_cost[0, i-1] \n",
    "for i in range(1, len(y)):\n",
    "    for j in range(1, len(x)):\n",
    "        accumulated_cost[i, j] = min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]) \\\n",
    "                                + distances[i, j]\n",
    "path = path_cost(x, y, accumulated_cost, distances)[0]\n",
    "path_x = [point[0] for point in path]\n",
    "path_y = [point[1] for point in path]\n",
    "distance_cost_plot(accumulated_cost)\n",
    "plt.plot(path_x, path_y)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(x, 'bo-' ,label='x')\n",
    "plt.plot(y, 'g^-', label = 'y')\n",
    "plt.legend()\n",
    "paths, cost = path_cost(x, y, accumulated_cost, distances)\n",
    "print \"DTW cost: {}\".format(cost)\n",
    "for [map_x, map_y] in paths:\n",
    "    #print map_x, x[map_x], \":\", map_y, y[map_y]\n",
    "    \n",
    "    plt.plot([map_x, map_y], [x[map_x], y[map_y]], 'r')\n",
    "plt.title(\"Warping\")\n",
    "plt.suptitle('Difference between halves of windows {}'.format(windows_number), fontsize=21)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian changepoint detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import online_chan_detector as oncd\n",
    "from functools import partial\n",
    "\n",
    "windows_number = 11\n",
    "data = [x/16384.0 for x in wFrames[windows_number][\"accX\"].dropna()]\n",
    "R, maxes = oncd.online_changepoint_detection(data, partial(oncd.constant_hazard, 250), oncd.StudentT(0.1, .01, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "fig, ax = plt.subplots(figsize=[7, 7])\n",
    "ax = fig.add_subplot(3, 1, 1)\n",
    "ax.plot(data)\n",
    "ax.grid()\n",
    "ax1 = fig.add_subplot(3, 1, 2, sharex=ax)\n",
    "sparsity = 3 # only plot every fifth data for faster display\n",
    "ax1.pcolor(np.array(range(0, len(R[:,0]), sparsity)), \n",
    "          np.array(range(0, len(R[:,0]), sparsity)), \n",
    "          -np.log(R[0:-1:sparsity, 0:-1:sparsity]), \n",
    "          cmap=cm.Greys, vmin=0, vmax=30)\n",
    "ax2 = fig.add_subplot(3, 1, 3, sharex=ax)\n",
    "Nw=10;\n",
    "ax2.plot(R[Nw,Nw:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a PDF on the accelerometer and gyroscope data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pdf([w[\"accX\"].dropna().as_matrix() for w in wFrames], suptitle=\"AccX\")\n",
    "pdf([w[\"accY\"].dropna().as_matrix() for w in wFrames], suptitle=\"AccY\")\n",
    "pdf([w[\"accZ\"].dropna().as_matrix() for w in wFrames], suptitle=\"AccZ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pdf([w[\"gyroX\"].dropna().as_matrix() for w in wFrames], suptitle=\"gyroX\")\n",
    "pdf([w[\"gyroY\"].dropna().as_matrix() for w in wFrames], suptitle=\"gyroY\")\n",
    "pdf([w[\"gyroZ\"].dropna().as_matrix() for w in wFrames], suptitle=\"gyroZ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a pdf to the Microsoft Kinect 2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pdf([w[\"ci\"].dropna().as_matrix() for w in wFrames], suptitle=\"ci\")\n",
    "#pdf([w[\"distance\"].dropna().as_matrix() for w in wFrames], suptitle=\"distance\")\n",
    "#pdf([w[\"proximity\"].dropna().as_matrix() for w in wFrames], suptitle=\"proximity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting One-lag time graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gridOfLagPlots(wFrames, suptitle=\"One Lag scatters - accX\", columnToPlot=\"accX\")\n",
    "gridOfLagPlots(wFrames, suptitle=\"One Lag scatters - accY\", columnToPlot=\"accY\")\n",
    "gridOfLagPlots(wFrames, suptitle=\"One Lag scatters - accZ\", columnToPlot=\"accZ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gridOfLagPlots(wFrames, suptitle=\"One Lag scatters - gyroX\", columnToPlot=\"gyroX\")\n",
    "gridOfLagPlots(wFrames, suptitle=\"One Lag scatters - gyroY\", columnToPlot=\"gyroY\")\n",
    "gridOfLagPlots(wFrames, suptitle=\"One Lag scatters - gyroZ\", columnToPlot=\"gyroZ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gridOfLagPlots(wFrames, suptitle=\"One Lag scatters - ci\", columnToPlot=\"ci\")\n",
    "gridOfLagPlots(wFrames, suptitle=\"One Lag scatters - distance\", columnToPlot=\"distance\")\n",
    "gridOfLagPlots(wFrames, suptitle=\"One Lag scatters - proximity\", columnToPlot=\"proximity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting an autoregressive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train autoregression\n",
    "X = wFrames[0][\"accX\"].dropna().as_matrix()\n",
    "train, test = X[1:len(X) - 7], X[len(X) - 7:]\n",
    "model = AR(train)\n",
    "model_fit = model.fit()\n",
    "print('Lag: %s' % model_fit.k_ar)\n",
    "print('Coefficients: %s' % model_fit.params)\n",
    "# make predictions\n",
    "predictions = model_fit.predict(start=len(train), end=len(train) + len(test) - 1, dynamic=False)\n",
    "for i in range(len(predictions)):\n",
    "    print('predicted=%f, expected=%f' % (predictions[i], test[i]))\n",
    "error = mean_squared_error(test, predictions)\n",
    "print('Test MSE: %.3f' % error)\n",
    "# plot results\n",
    "plt.figure(\"AR Model\")\n",
    "plt.plot(test)\n",
    "plt.plot(predictions, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fast Fourrier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gridOfFFT(data, N_fft, freq_range=64,title=\"Plot\", columnToPlot = None):\n",
    "    grid_side_size = int(round(np.sqrt(len(data))))\n",
    "    plt.figure(title, figsize=(18,12))\n",
    "\n",
    "    Fs = freq_range\n",
    "    count = 0\n",
    "    for i in range(grid_side_size):\n",
    "        for j in range(grid_side_size):\n",
    "            if count >= len(data):\n",
    "                break\n",
    "            ax = plt.subplot(grid_side_size, grid_side_size, count + 1)\n",
    "            \n",
    "            if columnToPlot in ['accX','accX','accX','gyroX','gyroX','gyroX']:\n",
    "                norm_data = [d/16384.0 for d in data[count][columnToPlot].dropna()]\n",
    "            else:\n",
    "                norm_data = data[count][columnToPlot].dropna()\n",
    "            \n",
    "            mean_sig = np.ones_like(norm_data)*np.mean(norm_data)\n",
    "            # remove mean of the signal, for better results.\n",
    "            norm_data = norm_data - mean_sig\n",
    "\n",
    "            freqsig = fft.fft(norm_data, n=N_fft)\n",
    "            freq_axis = np.arange(0, Fs, Fs / N_fft)\n",
    "\n",
    "            ax.plot(freq_axis, np.abs(freqsig), lw=2.0, c='b')\n",
    "            p = plt.Rectangle((Fs/2, 0), Fs/2, ax.get_ylim()[1], facecolor=\"grey\", fill=True, alpha=0.75, hatch=\"/\", zorder=3)\n",
    "            ax.add_patch(p)\n",
    "            #ax.set_xlim((ax.get_xlim()[0],Fs))\n",
    "            ax.set_xlim((-2,Fs))\n",
    "            plt.title(\"FFT - Windows{}\".format(count), fontsize=10)\n",
    "            plt.ylabel('FFT magnitude')\n",
    "            plt.xlabel('Frequency (Hz)')\n",
    "            plt.legend((p,), ('excluded',))\n",
    "            plt.grid()\n",
    "            count += 1\n",
    "\n",
    "    plt.suptitle(title, fontsize=21)\n",
    "    plt.subplots_adjust(hspace=0.8, wspace=0.8)\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gridOfFFT(wFrames, 200, title=\"FFT for the accX\", columnToPlot=\"accX\")\n",
    "\n",
    "#gridOfFFT(wFrames, 200, title=\"FFT for the accY\", columnToPlot=\"accY\")\n",
    "#gridOfFFT(wFrames, 200, title=\"FFT for the accZ\", columnToPlot=\"accZ\")\n",
    "#gridOfFFT(wFrames, 200, title=\"FFT for the accZ\", columnToPlot=\"accZ\")\n",
    "#gridOfFFT(wFrames, 200, title=\"FFT for the ci\", columnToPlot=\"ci\")\n",
    "#gridOfFFT(wFrames, 200, title=\"FFT for the proximity\", columnToPlot=\"proximity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate meta-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def flatten_dict(dd, separator='.', prefix=''):\n",
    "        \"\"\"This function collapses a dictionary into a list, by appending\n",
    "        the keys' values to themselves. That is, parents(keys) are joined together\n",
    "        with children (values) by the separator variable.\n",
    "        dd  :   dictionary to be flattened\n",
    "        separator   :   the character used to join values to their keys.\n",
    "        prefix      :   the character used in place of the value.\n",
    "        \"\"\"\n",
    "        return {prefix + separator + k if prefix else k: v\n",
    "                for kk, vv in dd.items()\n",
    "                for k, v in flatten_dict(vv, separator, kk).items()\n",
    "                } if isinstance(dd, dict) else {prefix: dd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-feature function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(data):\n",
    "    \"\"\"Calculates mean of the data\"\"\"\n",
    "    if isinstance(data,list):\n",
    "        return [d.mean(axis=0, skipna=True, numeric_only=True) for d in data]\n",
    "    else:\n",
    "        return data.mean(axis=0, skipna=True)\n",
    "\n",
    "\n",
    "def std(data):\n",
    "    \"\"\"Calculates the standard deviation\"\"\"\n",
    "    if isinstance(data,list):\n",
    "        return [d.std(axis=0, skipna=True, numeric_only=True) for d in data]\n",
    "    else:\n",
    "        return data.std(axis=0, skipna=True, numeric_only=True)\n",
    "\n",
    "\n",
    "def max_value(data):\n",
    "    \"\"\" Calculates Largest value in array\"\"\"\n",
    "    if isinstance(data,list):\n",
    "        return [d.max(axis=0, skipna=True, numeric_only=True) for d in data]\n",
    "    else:\n",
    "        return data.max(axis=0, skipna=True, numeric_only=True)\n",
    "\n",
    "    \n",
    "def min_value(data):\n",
    "    \"\"\"Calculates smallest value in array\"\"\"\n",
    "    if isinstance(data,list):\n",
    "        return [d.min(axis=0, skipna=True, numeric_only=True) for d in data]\n",
    "    else:\n",
    "        return data.min(axis=0, skipna=True, numeric_only=True)\n",
    "\n",
    "\n",
    "def mad(data):\n",
    "    \"\"\" Calculates the median absolute deviation\"\"\"\n",
    "    if isinstance(data,list):\n",
    "        l = []\n",
    "        for d in data:\n",
    "            m = {}\n",
    "            for k in data._get_numeric_data():\n",
    "                m[k] = abs(data[k].dropna() - data[k].median())\n",
    "                m[k] = pd.Series(m[k]).median()\n",
    "            l.append(pd.Series(m))\n",
    "        return l\n",
    "    else:\n",
    "        m = {}\n",
    "        for k in data._get_numeric_data():\n",
    "            m[k] = abs(data[k].dropna() - data[k].median())\n",
    "            m[k] = pd.Series(m[k]).median()\n",
    "        return pd.Series(m)\n",
    "\n",
    "\n",
    "def sma(data):\n",
    "    \"\"\"Computes Signal magnitude area.\n",
    "    http://dsp.stackexchange.com/questions/18649/signal-magnitude-area\n",
    "    \"\"\"\n",
    "    if isinstance(data,list):\n",
    "        return [d.sum(axis=0, skipna=True, numeric_only=True) / data.shape[0] for d in data]\n",
    "    else:\n",
    "        return data.sum(axis=0, skipna=True, numeric_only=True) / data.shape[0]\n",
    "\n",
    "\n",
    "def energy(data):\n",
    "    \"\"\"Energy measure. Sum of the squares divided by the number of values.\"\"\"\n",
    "    if isinstance(data,list):\n",
    "        return [d.dropna().apply(lambda x: x**2).mean(axis=0, skipna=True, numeric_only=True) for d in data]\n",
    "    else:\n",
    "        return data.dropna().apply(lambda x: x**2).mean(axis=0, skipna=True, numeric_only=True)\n",
    "\n",
    "\n",
    "def iqr(data):\n",
    "    \"\"\"Calculates the interquartile range\n",
    "    http://stackoverflow.com/questions/23228244/how-do-you-find-the-iqr-in-numpy\n",
    "    \"\"\"\n",
    "    if isinstance(data,np.ndarray):\n",
    "        return np.subtract(*np.percentile(data, [75, 25]))\n",
    "    else:\n",
    "        v = {}\n",
    "        for k in data._get_numeric_data():\n",
    "            v[k] = np.subtract(*np.percentile(data[k].dropna(), [75, 25]))\n",
    "        return pd.Series(v)\n",
    "\n",
    "def entropy(data):\n",
    "    \"\"\"Signal entropy\"\"\"\n",
    "    pass\n",
    "\n",
    "def maxInds(data, n_bins=200, filterMean= True):\n",
    "    \"\"\"Returns the index of the frequency component with largest magnitude\"\"\"\n",
    "    m_indexes = {}\n",
    "    for k in data._get_numeric_data():\n",
    "        c_sig = []\n",
    "        if filterMean:\n",
    "            filtered = data[k].dropna().as_matrix()\n",
    "            mean_sig = np.ones_like(filtered)*np.mean(filtered)\n",
    "            # remove mean of the signal, for better results.\n",
    "            c_sig = data[k].dropna().as_matrix() - mean_sig\n",
    "        freqsig = fft.fft(c_sig,n=n_bins) \n",
    "        half_freq_domain = freqsig[:int(n_bins/2)]\n",
    "        #get index in the freq domain\n",
    "        m_indexes[k] = np.where(np.abs(half_freq_domain)==(max(np.abs(half_freq_domain))))[0][0]\n",
    "    return pd.Series(m_indexes)\n",
    "\n",
    "def meanFreq(data, n_bins=200, filterMean=True):\n",
    "    \"\"\"\n",
    "    Weighted average of the frequency components to obtain a mean frequency\n",
    "    http://luscinia.sourceforge.net/page26/page35/page35.html\n",
    "    \"\"\"\n",
    "    m_freq = {}\n",
    "    for k in data._get_numeric_data():\n",
    "        c_sig = []\n",
    "        if filterMean:\n",
    "            filtered = data[k].dropna().as_matrix()\n",
    "            mean_sig = np.ones_like(filtered)*np.mean(filtered)\n",
    "            # remove mean of the signal, for better results.\n",
    "            c_sig = data[k].dropna().as_matrix() - mean_sig\n",
    "        freqsig = fft.fft(c_sig,n=n_bins) \n",
    "        half_freq_domain = freqsig[:int(n_bins/2)]\n",
    "        #get index in the freq domain\n",
    "        m_freq[k] = np.sum(np.abs(half_freq_domain) * range(len(half_freq_domain)))/sum(np.abs(half_freq_domain))\n",
    "    return pd.Series(m_freq)\n",
    "\n",
    "def skewness(data, n_bins=200, filterMean=True): \n",
    "    \"\"\"skewness of the frequency domain signal\"\"\"\n",
    "    m_skew = {}\n",
    "    for k in data._get_numeric_data():\n",
    "        c_sig = []\n",
    "        if filterMean:\n",
    "            filtered = data[k].dropna().as_matrix()\n",
    "            mean_sig = np.ones_like(filtered)*np.mean(filtered)\n",
    "            # remove mean of the signal, for better results.\n",
    "            c_sig = data[k].dropna().as_matrix() - mean_sig\n",
    "        freqsig = fft.fft(c_sig,n=n_bins) \n",
    "        half_freq_domain = freqsig[:int(n_bins/2)]\n",
    "        #get index in the freq domain\n",
    "        m_skew[k] = skew(c_sig)\n",
    "    return pd.Series(m_skew)\n",
    "\n",
    "def kurtos(data, n_bins=200, filterMean=True):\n",
    "    \"\"\"kurtosis of the frequency domain signal\"\"\"\n",
    "    m_kurtosis = {}\n",
    "    for k in data._get_numeric_data():\n",
    "        c_sig = []\n",
    "        if filterMean:\n",
    "            filtered = data[k].dropna().as_matrix()\n",
    "            mean_sig = np.ones_like(filtered)*np.mean(filtered)\n",
    "            # remove mean of the signal, for better results.\n",
    "            c_sig = data[k].dropna().as_matrix() - mean_sig\n",
    "        freqsig = fft.fft(c_sig,n=n_bins) \n",
    "        half_freq_domain = freqsig[:int(n_bins/2)]\n",
    "        #get index in the freq domain\n",
    "        m_kurtosis[k] = kurtosis(c_sig)\n",
    "    return pd.Series(m_kurtosis)\n",
    "\n",
    "def bandsEnergy():\n",
    "    \"\"\"Energy of a frequency interval within the bins of the FFT.\"\"\"\n",
    "    pass\n",
    "\n",
    "def angle():\n",
    "    \"\"\"Angle between to vectors.\"\"\"\n",
    "    pass\n",
    "\n",
    "def arCoeff(): \n",
    "    \"\"\"Autorregresion coefficients with Burg order equal to 4\"\"\"\n",
    "    pass\n",
    "\n",
    "def correlation_acc(data): \n",
    "    \"\"\"correlation coefficient between two accelerometer signals\"\"\"\n",
    "    \n",
    "    cor = data[[\"accY\", \"accX\", \"accZ\", \"gyroZ\", \"gyroX\", \"gyroY\"]].corr().to_dict()['accY']\n",
    "    res = {}\n",
    "    for k,v in cor.iteritems():\n",
    "        if k == 'accY':\n",
    "            continue\n",
    "        res[k+'-accY'] = v\n",
    "    return res\n",
    "\n",
    "def correlation_kinect(data): \n",
    "    \"\"\"correlation coefficient between two accelerometer signals\"\"\"\n",
    "    cor = flatten_dict(data[[\"ci\",\"proximity\"]].corr().to_dict()[\"ci\"])\n",
    "    res = {}\n",
    "    for k,v in cor.iteritems():\n",
    "        if k == 'ci':\n",
    "            continue\n",
    "        res[k+'-ci'] = v\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating metafeatures for one single window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class tabularData():\n",
    "    def __init__(self):\n",
    "        self.rows = defaultdict(list)\n",
    "        self.headers = ['topics']\n",
    "\n",
    "    def store(self,d,func):\n",
    "        for k,v in d.items():\n",
    "            self.rows[k].append(float(\"{:.2f}\".format(v)))\n",
    "        self.headers.append(func)\n",
    "        \n",
    "    def getData(self):\n",
    "        r = []\n",
    "        for k,v in self.rows.items():\n",
    "            r.append([k] + [i for i in v])\n",
    "        return sorted(r)\n",
    "    \n",
    "    def getHeaders(self):\n",
    "        return self.headers\n",
    "\n",
    "data = wFrames[0]._get_numeric_data()\n",
    "\n",
    "table_rows = tabularData()\n",
    "table_rows.store(mean(data).to_dict(), \"mean\")\n",
    "table_rows.store(std(data).to_dict(), \"std\")\n",
    "table_rows.store(max_value(data).to_dict(), \"max\")\n",
    "table_rows.store(min_value(data).to_dict(), \"min\")\n",
    "table_rows.store(mad(data).to_dict(), \"mad\")\n",
    "table_rows.store(sma(data).to_dict(), \"sma\")\n",
    "table_rows.store(iqr(data).to_dict(), \"iqr\")\n",
    "table_rows.store(energy(data).to_dict(), \"energy\")\n",
    "table_rows.store(maxInds(data).to_dict(), \"maxInds\")\n",
    "table_rows.store(meanFreq(data).to_dict(), \"meanFreq\")\n",
    "table_rows.store(skewness(data).to_dict(), \"skewness\")\n",
    "\n",
    "table_rows2 = tabularData()\n",
    "table_rows2.store(kurtos(data).to_dict(), \"kurtosis\")\n",
    "\n",
    "print 115 * '*'\n",
    "print 45*' '+\"META-FEATURES - 1 windows\"\n",
    "print 115 * '*'\n",
    "print    \n",
    "print tabulate(table_rows.getData(), headers=table_rows.getHeaders(), numalign=\"right\")\n",
    "print 115 * '-'\n",
    "print tabulate(table_rows2.getData(), headers=table_rows2.getHeaders(), numalign=\"right\")\n",
    "\n",
    "print json.dumps(correlation_acc(data), indent=4)\n",
    "print json.dumps(correlation_kinect(data),indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "win_number = 1\n",
    "\n",
    "sig = [s/16384.0 for s in wFrames[win_number][\"accZ\"].dropna()]\n",
    "N_samps = len(sig)\n",
    "time_interval = 3#secs\n",
    "Fs = len(sig)/time_interval           # the range of frequency to look for.\n",
    "N_fft = 200                           # Number of bins (chooses granularity)\n",
    "T = 1./Fs                             #the period, the sample time, the time after which each data come.\n",
    "#t  = np.linspace(0,N_samps*T,N_samps) # N_samps*T (#samples x sample period) is the signal time.\n",
    "\n",
    "mean_sig = np.ones_like(sig)*np.mean(sig)\n",
    "# remove mean of the signal, for better results.\n",
    "sig = sig - mean_sig\n",
    "\n",
    "freqsig = fft.fft(sig,n=N_fft) \n",
    "# ATTENTION: if the ratio Fs/N_fft is equal to 1, set the start of np.arange to 1, otherwise, to zero.\n",
    "freq_axis = np.arange(0,Fs,Fs/N_fft)\n",
    "\n",
    "fig = plt.figure(figsize=[14,7])\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 3]) \n",
    "plt.subplots_adjust(hspace=0.5, wspace= 0.4)\n",
    "\n",
    "##### Plot the fft #####\n",
    "ax = plt.subplot(221)\n",
    "pt, = ax.plot(freq_axis,np.abs(freqsig), lw=2.0, c='b')\n",
    "p = plt.Rectangle((Fs/2, 0), Fs/2, ax.get_ylim()[1], facecolor=\"grey\", fill=True, alpha=0.75, hatch=\"/\", zorder=3)\n",
    "ax.add_patch(p)\n",
    "ax.set_xlim((ax.get_xlim()[0],Fs))\n",
    "ax.set_title('FFT - Windows {} signal'.format(win_number), fontsize= 16, fontweight=\"bold\")\n",
    "ax.set_ylabel('FFT magnitude (power)')\n",
    "ax.set_xlabel('Frequency (Hz)')\n",
    "plt.legend((p,), ('excluded',))\n",
    "ax.grid()\n",
    "\n",
    "##### Close up on the graph of fft#######\n",
    "# This is the same histogram above, but truncated at the max frequence + an offset. \n",
    "offset = 1    # just to help the visualization. Nothing important.\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax2.plot(freq_axis,np.abs(freqsig), lw=2.0, c='b')\n",
    "#plt.yscale('log')\n",
    "ax2.set_xticks(freq_axis)\n",
    "ax2.set_xlim(0,int(Fs/24)+offset)\n",
    "ax2.set_title('FFT close up', fontsize= 16, fontweight=\"bold\")\n",
    "ax2.set_ylabel('FFT magnitude (power)')\n",
    "ax2.set_xlabel('Frequency (Hz)')\n",
    "ax2.hold(True)\n",
    "ax2.grid()\n",
    "\n",
    "##### Close up on the graph of fft with hanning windowing function#######\n",
    "# This is the same histogram above, but truncated at the max frequence + an offset. \n",
    "offset = 1    # just to help the visualization. Nothing important.\n",
    "sig = sig * np.hanning(len(sig))\n",
    "freqsig = fft.fft(sig,n=N_fft) \n",
    "ax3 = fig.add_subplot(223)\n",
    "ax3.plot(freq_axis,np.abs(freqsig), lw=2.0, c='b')\n",
    "#plt.yscale('log')\n",
    "ax3.set_xticks(freq_axis)\n",
    "ax3.set_xlim(0,int(Fs/24)+offset)\n",
    "ax3.set_title('FFT close up with Hanning', fontsize= 16, fontweight=\"bold\")\n",
    "ax3.set_ylabel('FFT magnitude (power)')\n",
    "ax3.set_xlabel('Frequency (Hz)')\n",
    "ax3.hold(True)\n",
    "ax3.grid()\n",
    "\n",
    "#print maxInds(sig,200, filterMean= True)\n",
    "#print meanFreq(sig,200, filterMean= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "failed_files = []\n",
    "\n",
    "for csv_filename in files:\n",
    "    csv_data = None          # the variable where the loaded csv data is stored.\n",
    "    num_windows = 0          # the number of windows loaded.\n",
    "    wFrames = []             # the list of windows data. Each element is a pandas dataframe \n",
    "                             #  corresponding to the windows. The list is of size 'num_windows'.\n",
    "    print\n",
    "    print 30*'='\n",
    "    print 'Processing: \"{}\"'.format(csv_filename)\n",
    "\n",
    "    # load the data, abort in case of error.\n",
    "    try:\n",
    "        num_windows, csv_data = getCSV(os.path.join(csv_dir, csv_filename))\n",
    "    except ValueError as e:\n",
    "        print traceback.format_exc()\n",
    "        sys.exit(-1)\n",
    "\n",
    "    for w in range(num_windows):\n",
    "        win_data = {}\n",
    "        for k in csv_data.keys():\n",
    "            # consider the data only if it is not in the ignore list.\n",
    "            if k not in ignore_col_list:\n",
    "                win_data[feature_name_map[k]] = csv_data[k][w]\n",
    "\n",
    "        # convert dictionary to dataframe and save it to list of all windows data for the file.\n",
    "        wFrames.append(pd.DataFrame.from_dict(win_data))\n",
    "    print 'Retrieved {} windows in {}'.format(num_windows, csv_filename)\n",
    "    overlap_reference = 50\n",
    "    \n",
    "    try:\n",
    "        _, n_windows, sample_info, avg_overlap, avg_diff = getStatistics(csv_data, compareWith=overlap_reference)\n",
    "        print \"LOAD SUMMARY:\"\n",
    "        print tabulate([[n_windows,\"{:.2f}\".format(avg_overlap),\"{:.2f}\".format(avg_diff)]],\n",
    "                           headers=[\"#Win\", \"Avg. Overlap\", \"Avg. dev. from ref.\"])\n",
    "    except ValueError as e:\n",
    "        print traceback.format_exc()\n",
    "        failed_files.append(csv_filename)\n",
    "        continue\n",
    "    \n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
