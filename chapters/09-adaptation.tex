\chapter{A theoretical model for adjusting robot playing behavior}\label{ch:adaptation}

The ultimate goal in a~\gls{pirg} application is to be able to present some degree of behavior personalization as a mean to increase entertainment. Before achieving this goal, as previous chapters demonstrate, one must provide the basic game capabilities. Considering that other phases of design are completed and the game is able to provide a basic experience, it is then possible to study how to effectively tailor the behavior of the robot to the individual human player so as to present an engaging interaction in terms of an appropriate game difficulty. This can be achieved by the modulation of the robot's ability to play considering the existence of a set of discrete styles each one of which presenting its own difficulty. These styles can be viewed as difficulty settings from which an appropriated one is selected for the player at hand. This is the main idea in strategies for~\glsdesc{dda}, where game designers define approaches to select which game difficulty (\eg, easy, medium, hard) are ideal. 

In our game, the only way to control difficulty is to change properties in the robot behavior, such as speed, and, thus, the difficulty perception is affected only by the modulation of its properties. It is possible that, in other~\gls{pirg}, other elements external to the robot affect the difficulty perception, making an appropriate difficulty selection for the game more complex. Such types of~\gls{pirg}s are outside of our research scope, but the insights we provide can be extended to such cases. 

In our approach, the effective game difficulty encountered by players is estimated using latent skill representation through the definition of a similarity space. Encoding players and difficulty parameters into such space allows for adaptation, by turning the problem into a recommendation one. Driving the difficulty towards an appropriated level is extensively reported in the literature as responsible for impacting player satisfaction and fun. %TODO Insert citations here
We also provide observations for robogame designers via the analysis of a theoretical ``risk-ability'' space. This chapter begins with the problem definition.

\section{Problem definition}
We want to exploit past interaction in order to select an ideal robot behavior to be used against the user at hand, thus, hopefully increasing his entertainment. The selected behavior is intended to be one maximizing the entropy of a distribution representing the probability of victory for the player. For this binary probability mass function, which can be thought of as a Bernoulli distribution, the probability of success, $\rho$, indicates the chances of victory for the player and, conversely, $1-\rho$ indicates the chances of the player losing the game. We hypothesize that the entropy of such binary distribution can be used as a criteria for the task of selecting a balanced behavior, and that this happens when we are not sure which event is expected to occur. In other words, when the entropy of the distribution is high. %TODO Is this correct? Is the entropy of the distribution?. EWERTON: I Think so. For a binary distribution, maximum uncertainty occur when the two outcomes are equally likely.
Notice that at the beginning, the distribution can have high entropy due to the lack of information. However, as the number of plays increase, the entropy is expected to settle around their true values for all players.

In the process of selecting the appropriate behavior, we assume the game designer has defined a number of distinct robot parameters which the system can choose from. These parameters are supposed to be related to the game difficulty so as to allow some control over it. In particular, we assume the designer has divided the range of each parameter values into meaningful discrete values, such that it is possible to obtain a finite set $M$ of difficulty-related parameters, henceforth called~\gls{ds}. For instance, in RoboTower 2.0, a parameter related to the difficulty of the game is the maximum velocity of the robot, which can be used to control the amount of physical activity the human player will engage against. Another parameter is the blocking factor, which essentially defines the tolerance of the robot when competing for a tower. A joint assignment of such variables makes up for a~\gls{ds}. 

One perspective for solving the problem is to take advantage from parametric modeling ideas regarding latent skill modeling. In particular, we tackle the problem from the perspective of jointly modeling players and difficulty parameters into a latent space such that it makes it easy to search for balanced settings. These characteristics make our methodology bear some similarity with recommender systems where the item being recommended, in this case, is a game level configuration such that the estimated fun is maximized. 

This way of posing the problem,\ie by having discrete behaviors and appealing for some mechanism for selecting them, had been proposed before in the work of~\cite{sejrsgaard-jacobsen_dynamic_2011}, and can be motivated by the fact that we can separate behavior definition from adaptation. This can also favor other aspects system development, like modularity and maintenance.

Formally, for a player $p$ and a set of $M$ distinct~\gls{ds} $\{m_{j}\}^{j=M}_{j=1}$ we aim at choosing a setting $m^{\star}$ such that:

\begin{equation}
m^{\star} = \argmax_{m \in \mathcal{M}} \mathcal{G}(m, p, f_{p}),
\end{equation}
where $\mathcal{G}$ represents the game, $f_{p}$ features of the player, and $m^{\star}$ the best game~\gls{ds} $m$. %TODO What about p? If it is the player it is important to make the reader noticing the dependency. EWERTON: I just included that the fuctions depends on the player and his score.
In the next section we discuss our proposal to solve for $m^{\star}$.

\section{Quantifying difficulty}\label{sec:quant_difficulty}
The notion of difficulty is related to several variables. To the extent of a game, it is mostly related to factors like: game controllability, motor coordination, cognitive load management, memory, reasoning, and several others. Although difficulty in itself accounts for all such aspects at once, one usually considers to control a particular subset of variables, and try to understand up to which point their correlation may alter the perceived entertainment.

Following this, we investigate the relation between the dimensions of \textit{progress} and \textit{effort}. The former relates to the quantification of the amount of game progress a given player is likely to have when facing a given difficulty level. %TODO Wouldn't be "performance" more direct than "progress"
This is usually measured in relation with the game score, the state evaluation (as standard practice in games like chess), or even statistics from previous plays. Effort, in turn, relates to the amount of ``resource expenditure'' by the player as, for example, the cognitive effort, the number of actions taken, the number of in-game resources wasted, etc. In other words, it is designed to quantify the amount of energy spent when playing.

We consider the relationship between effort and progress as providing enough information to support the selection of an appropriate level of play. One obstacle, however, is in the fact that effort does not generally offer a linear relationship with perceived difficulty and it is not straightforward to assume that difficulty is inversely proportional to it,~\ie, it is not always true that a higher resource investment means less ability to play. 
Nonetheless, it is reasonable to assume that players do not conscientiously obtain more entertainment, or advantage, by allocating unnecessary efforts. Following such assumption, one may view a skilled player as one that is constantly trying to minimize energy expenditure while maximizing progress, which supports the hypothesis that players are reasonably classified by considering their progress and effort.

\section{A proposed model: Collaborative effort regression}

Among the most successful techniques for recommender system is~\gls{cf}~\citep{su_survey_2009, schafer_collaborative_2007}. %TODO Insert citation. EWERTON: Done.
This technique is able to make automatic predictions about the interests of a user by collecting preferences from many others. The underlying assumption of the method is that if a user X has the same opinion as a user Y on an issue, X is more likely to have Y's opinion on a different issue than that of a randomly chosen user. In general, the technique differs from the simpler approach of giving an average (non-specific) preference, since it glean information from many users when exploiting their similarities.

From the class of~\gls{cf} approaches, one may observe the potential advantage of~\gls{pmf}~\citep{mnih_probabilistic_2008}. This approach was invented as a type of matrix factorization method. Essentially, it models the user's preference matrix (a matrix of scores, votes, etc.) as a product of two lower-rank user and item matrices. Mathematical details regarding the standard~\gls{pmf} is given in appendix~\ref{app:pmf}. 

In general, in the recommendation by matrix factorization, one represents the users and the items being recommended in a shared latent space of dimension K, where a user $i$ is represented by a latent vector $u_{i}$ defined in this space, \ie $u_{i} \in \mathbb{R}_{K}$ and a item $j$, in turn, by its own vector $v_{j} \in \mathbb{R}_{K}$. The predictions of whether the user $i$ is going to like item $j$ is then modeled by the inner product of their latent vectors.

\begin{equation}
    s_{ij} = u_{i}^{T}v_{j}
\end{equation}

The common approach to estimate the latent vectors is to minimize the regularized squared error loss with respect to all users $U=(u_{i})_{i=1}^{I}$ and $V=(v_{i})_{j=1}^{J}$, with $\lambda_{u}$ and $\lambda_{v}$ as regularized terms (appendix~\ref{app:pmf}). For the probabilistic version of matrix factorization it is assumed the following generative process~\citep{wang_collaborative_2011}, where $I_{K}$ is a K-dimensional identity matrix.

\begin{enumerate}
    \item For each user $i$, draw user latent vector $u_{i} \sim \mathcal{N}(0,\lambda_{u}^{-1}I_{K})$.
    \item For each item $j$, draw user latent vector $v_{j} \sim \mathcal{N}(0,\lambda_{v}^{-1}I_{K})$.
    \item For each user-item ($i$, $j$), draw the response $s_{ij} \sim \mathcal{N}(u_{i}^{T}v_{j}, c_{ij}^{-1})$
\end{enumerate}


Concerning our scenario of interest, one can conveniently arrange the score data from players in a matrix form, with rows indicating the player and columns the score when playing against a particular~\gls{ds}. The objective is then to probabilistic model the scores by estimating low-rank matrices associated with users and low-rank matrices associated with each~\gls{ds}. %TODO You mean "so to model probabilistically the score matrix"? Does this make sense?. EWERTON: Fixed the parameter.
Following the standard approach in~\gls{pmf}, this can be a linear model with Gaussian observation noise. 

The~\gls{pmf} model, in its standard form, cannot incorporate the role of effort and the quantification of difficulty discussed in section~\ref{sec:quant_difficulty}. One alternative to this issue is to reuse the~\gls{lda} model from chapter~\ref{ch:modeling} in order to account for different types of effort. The mixture proportions learned in this model, which categorize a player as a combination of effort types uncovered from the data, can be used to represent players and be combined with the latent variables for the~\gls{ds} in the generative process of the score matrix.

It turns out that a model of this type was proposed in~\cite{wang_collaborative_2011} and was named as~\gls{ctr}. It combines collaborative filtering and topic modeling in order to fit a model that uses the latent topic space to explain both the observed ratings (votes for an item) and the observed words from the item descriptions. 

The~\gls{ctr} represents users with topic interests and assumes that item description are generated by a topic model. The model additionally includes a latent variable, $\epsilon$, that offsets the topic proportions when modeling the user ratings. the more user rates are available, the more the offset becomes important in explaining preference nuances.

%Below, we present the derivation of important variables of this model in our domain, here termed~\gls{cer}. Details regarding the general applicability of~\gls{ctr} can be viewed in~\cite{wang_collaborative_2011}.

As before, for our context, we consider $M$ robot difficulty settings, $N$ players and a matrix $S_{ij}$ representing the score of player $i$ when playing against a~\gls{ds} $m_{j}$. We also define $U \in \mathbb{R}^{D\times N}$ and $V \in \mathbb{R}^{D\times M}$ as the latent player and~\gls{ds} feature matrices, with column vectors $U_i$ and $V_j$ representing player-specific and~\gls{ds}-specific latent feature vectors respectively. 

Since~\gls{ctr} uses~\gls{lda} as sub-component, we also consider a variable $\xi$ representing player effort. This variable can be, as suggested, described as the actions the player performs when playing. By reusing~\gls{lda} the model attempts to capture the player behavior as a collection of actions whose modeling variables has some correlation with the game score. Similar to the exposed in chapter~\ref{ch:modeling}, in the context of the~\gls{lda}, such collection would be a document, where words are the actions performed by the player. Naturally, the job of the~\gls{lda} is to separate the documents into coherent groups all in terms of actions frequency and diversity. 

To our score, we call the topics find by~\gls{lda} \textit{effort topics} and we rename the entire model as~\gls{cer} in order to accommodate it in the context of our application.
%S%TODO This is quite obscure here. We are very far from LDA. At least put a reference... EWERTON: actually we are not. The model, as explained in equation for the joint has LDA as sub-component.
The joint likelihood of variables in~\gls{cer} can then be divided in two parts: the~\gls{lda} and the~\gls{pmf}:

\begin{equation}
    p(U,V,\Theta|S,\xi,\sigma^{2},\sigma_{U}^{2},\sigma_{V}^{2}) = \underbrace{p(Z,B, \Theta | \xi)}_{\gls{lda}} \underbrace{p(U,V|S,\Theta)}_{\gls{pmf}},
\end{equation}
where $\Theta$ represents the players effort mixture proportions, $B$ the effort topics and $Z$ the topic index variable. %TODO Can we shift here from topics to players, so that everything is uniform and more clear? %EWERTON: Actually no, players are different from topics. The reader needs to keep in mind the background of LDA from chapter\ref{ch:modeling} in order to understand the proposal.

Given topic parameters, the full posterior of the variables $u_{i}$, $v_{j}$ and $\theta_{j}$ is intractable. A~\gls{map} estimate can be learned using a~\gls{em}-based algorithm for the maximization of the log likelihood of U, V, $\Theta$ and S given $\lambda_{u}$, $\lambda_{v}$ and $B$. The algorithm proposed in~\cite{wang_collaborative_2011} can still be used to learn the variable in~\gls{cer}. 

The key aspect we would like to explore from the~\gls{ctr} model is how the latent vector of the user is taken to be $u_{i} \sim \mathcal{N}(\theta_{i}, \lambda_{u}^{-1}I_{K})$ %TODO here we are under a "how". What would you mean? The next sentence should be separated or rephrased to match the "how" grammar construction % EWERTON: we are interested in model the u_{i}, thus, we are interested in "how" it comes to be.
. In the process, it is assumed $u_{i}$ is close to the topic proportions $\theta_{i}$, which for us represents the proportion of effort topics. The expectation of the score signal, $s_{ij}$, under the model is a linear function of the $\theta_{i}$.

\begin{equation}
    \mathbb{E}[s_{ij}|u_{i}, \theta_{i}, \epsilon_{i}] = (\theta_{i} + \epsilon)v^{T},
    \label{eq:expect_ctr}
\end{equation}
where $\epsilon_{i}$ is the offset associated with the mixture proportion of efforts $\theta_{i}$ and it is distributed according to a zero-mean Gaussian with variance equal to $\lambda_{v}^{-1}$. For the effective selection of the new~\gls{ds}, one may estimate appropriate criterions from the expectation in equation~\ref{eq:expect_ctr}.

%EWERTON: I think that putting the derivation of the equations here is going to make the chapter too heavy. After all, the algorithm is the same with the variables changed for our context. My proposal to reuse this algorithm, however, inverts the role of the LDA: While the CTR uses a LDA for the recommended items, I propose to use it on the user description. However, the algorithm has minor changes and one can fully understand it by reading the original CTR in~\cite{wang_collaborative_2011}.
%TODO OK, but explain well what are the differences and what is your contribution

\section{Considerations}
Although in this chapter we do not provide any experimental result, we intended to give an overview of our current modeling assumption and thought process on how to adapt the robot behavior toward increasing the player entertainment.

We have framed our problem as a recommendation process into which the items being recommended are the possible robot~\glsdesc{ds}s and we ideally search for the one that is expected to be balanced w.r.t the chances of winning by both players, \ie human and robot alike. The score signal modeled is not limited to a scalar score, but can be redefined as a much general utility function that can map the game and player behavioral features to a scalar. 

The model presented above can also be seen from the perspective of putting together ideas from our previous experiments (such as those from chapter~\ref{ch:modeling}) in a much comprehensive scenario, extending further the contribution of our research.