\chapter{Hardware}\label{ch:foundation}
\epigraph{For the things we have to learn before we can do them, we learn by doing them.}{--- Aristotle, The Nicomachean Ethics}
\section{Game environment}\label{sec:game_environment}
As mentioned in section~\ref{sec:research_question}, we were interested in understanding phenomena related to player engagement in~\gls{pirg}s with a mobile adversarial robot. For that, we had chosen to use RoboTower~\citep{bonarini_timing_2014} as a starting point, perfecting the interaction and playability.

A main difference concerns the playground, which in our case consisted of a rectangular area of 4m$\times$4m, and the extinction of cards as mean of interaction. Here, we used the player's proximity instead as interaction channel. On each corner of the playground, tubes (henceforth called ``towers'') were placed (details in figure~\ref{fig:towers}). Each tower was equipped with a button (which sits on the tower's cap) and four~\gls{led}s that could be progressively turned on, one by one.  Each~\gls{led} required the button to be pressed for 2.5 seconds, meaning that the tower took about 10 seconds of button push in order to light up all of its four~\gls{led}s.

The~\gls{led}s are supposed to display the progress of the human player in capturing a specific tower. For a given tower, after turning on all~\gls{led}s, it is said that the player had captured it. When a tower is secured, the robot cannot aim at it anymore.
Button pressing time was cumulative and could be distributed on different moments -- that meant that the player would not lose his progress if he stopped pressing the button before the tower is completely captured. 

The game mechanics and winning conditions were simple enough to allow for a large number of individual playing the game. In order to win, the human players must be able to secure all the existing towers without letting a single one be knocked down by the robot. If, at anytime, a tower falls (because of the robot or the player) the game ends and the human player loses. 

The robot was able to move across the entire playground just as the human player could and it was only constrained by the fact that an already captured tower, or one whose button is currently being pressed by the player, cannot be teared down. As main interaction channel between the two players,\ie robot and human, the human player could block the robot path at any moment by staying in front of it, causing the later to likely change target tower. Therefore, as consequence of the defined rules, while the player was trying to capture a given tower, the robot could try to tear down any other one.

%ANDY This would go in the robot description -> By relying on the lasers scans the robot can perceive its environment, locate itself and the human player during the game, being able to perform obstacle avoidance when appropriated.

The game definition in itself is no trivial task and it is, for the case of~\gls{pirg}, limited by the constraints imposed by the mechanical platform,~\ie the robot. Other than playability and fun, safety is also an important aspect and one that impose heavy bounds to the motion of the robot: a too fast of a robot would reduce the perception of the robot been safe too play against, thus limit the interaction. Experimentally, we have evaluated that a robot with a maximum linear speed of 1.4 m/sec would be too difficulty to play against and/or invincible (which is not the behavior we were looking for). Intuitively, as we allowed an increase in speed, an impact in difficulty would be perceive. Moreover, for a mobile robot as ours, speed have some negative impact in the robot control itself, through mechanical phenomena like wheel slippage, initial control, manoeuvrability, and navigation issues (localization and obstacle avoidance). Details about the robot platform is given in section~\ref{sec:roboplat}.

In summary, our game, called RoboTower v2, was designed such that we could have an environment rich enough to physically engage human players while allowing for the study of adaptive approaches towards supporting such engagement. RoboTower v2 stimulates player to maintain strong cognitive tasks, like: trajectory planning, attention and spatial reasoning and places itself as an interesting environment from which one could test~\gls{ml} approaches. Next we briefly detail the Towers used in the game in term of their properties and operating system.
%ANDY Here is one of the places were the smartness of game design has to be put in evidence.

%\begin{figure}[H]
%	\centering
%	\begin{subfigure}[b]{0.4\textwidth}
%		\includegraphics[width=5cm]{Chapter4/Figs/situation1}
%		\caption{Initial situation: player is capturing Tower-4}
%		\label{situation1} 
%	\end{subfigure}
%	\begin{subfigure}[b]{0.4\textwidth}
%		\includegraphics[width=5cm]{Chapter4/Figs/situation2}
%		\caption{Robot start moving in order to attack and tear down Tower-2}
%		\label{situation2}
%	\end{subfigure}
%	\begin{subfigure}[b]{0.4\textwidth}
%		\includegraphics[width=5cm]{Chapter4/Figs/situation3}
%		\caption{Human player defends and start capturing Tower-2} 
%		\label{situation3}
%	\end{subfigure}
%	\begin{subfigure}[b]{0.4\textwidth}\centering
%		\includegraphics[width=3cm]{Chapter4/Figs/tubes}
%		\caption{Real towers that are used during the game}
%		\label{tubes} 
%	\end{subfigure}
%	\rule{35em}{0.5pt}
%	\caption{Schematic representation of the game:}
%	\label{overallgame}
%\end{figure}
%\begin{itemize}
%	\item figure~\ref{situation1}: The player is capturing the tower, when all the four LEDs are lit on the tower.
%	\item figure~\ref{situation2}: The robot attacks a Tower, it cannot try to tear down the tower that is currently being captured by the player.
%	\item figure~\ref{situation3}: The player stops the action of the robot by blocking its trajectory and defending the attacked tower. \textit{Please notice} how the progress on Tower-4 is not lost even if the player dismiss capturing it.
%\end{itemize}

%\begin{figure}[H]
%	\centering
%	\includegraphics[width=10cm]{Chapter5/Figs/im1}
%	\rule{35em}{0.5pt}
%	\caption{Moving robot tracking the movements of a human.}
%	\label{trackingconcept1} 
%\end{figure}

%To obtain the data from the human player that are needed to perform the tracking we used the Microsoft Kinect presented in~\ref{kinectsec} and a computer vision algorithm for blob detection previously integrated in the ROS environment using OpenCV libraries.

\section{Towers}\label{sec:towers}
Each tower was powered individually, being capable of transmitting its status to the robot at a constant rate. The circuit used the~\href{https://einstronic.com/wp-content/uploads/2017/06/NodeMCU-ESP8266-ESP-12E-Catalogue.pdf}{NodeMCU V3 ESP8266 ESP-12E} WiFi module\footnote{\url{https://goo.gl/TzAjwi} accessed on December 17th, 2018.} (see figure~\ref{fig:tower_electronics}) whose connection was done via a private network. The communication between towers and the robot was supported through~\gls{tcp} using the rosserial\_server\footnote{\url{http://wiki.ros.org/rosserial_server} accessed on December 17th, 2018.} package. Appendix~\ref{app:hard_appendix} provides additional hardware details for the towers. 

As power supply, a $7.4$V LiPo battery was used on each tower as can been seen on figure~\ref{fig:tower_electronics}. The nominal voltage for the boards was $5$V, which is supplied through a voltage regulator. A tilt sensor allows for the detection of fallen towers. A schematic of the circuit developed to detect such event is provided in figure~\ref{fig:tilt_circuit}.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
  	\centering
    \framebox{\parbox{3cm}{\includegraphics[width=3cm, height=4cm]{images/03-foundation/cap1}}}
	\caption{}
	\label{fig:tower_cap_top}
  \end{subfigure}
  ~ 
  \begin{subfigure}[b]{0.3\textwidth}
  	\centering
    \framebox{\parbox{3cm}{\includegraphics[width=3cm, height=4cm]{images/03-foundation/cap2}}}
	\caption{}
	\label{fig:tower_electronics}
  \end{subfigure}
  ~
   \begin{subfigure}[b]{0.3\textwidth}
	  \centering
      \framebox{\parbox{3cm}{\includegraphics[width=3cm,  height=4cm]{images/04-activity/tubes.jpg}}}
      \caption{}
    \end{subfigure}
  \caption{a) Tower cap containing the button (red square) and \gls{led}s; b) Tower cap electronic; c) The four towers used in the game (height 110cm).}
  \label{fig:towers}
\end{figure}

\section{The robotic platform}\label{sec:roboplat} 
As depicted in figure~\ref{graph:PIRG_design_structure}, hardware is the core concern for a mobile robot in~\gls{pirg}. In the graph in figure~\ref{graph:HARDWARE_structure} we have detailed some concepts of importance considered during our design. The graph is not supposed to give an extensive map of the necessary hardware aspects and their inter relationship, but to instruct the reader on the necessary aspect in this first phase of design taking our development as example. Naturally, not all mobile robots involved in~\gls{pirg}s will take into account all such concerns. In fact, the original RoboTower~\citep{bonarini_timing_2014} and Jedi Trainer 3.0~\citep{martinoia_physically_2013} were examples of mobile robots that did not considered, for instance, navigation techniques, such as~\gls{slam} and~\gls{oa} and had no sophisticated structural and sensing requirements given the little computing power available.

Our wheeled robot, instead, had holonomic kinematics and was called Triskar. Being holonomic, it was free to move in any direction at a speed comparable to that of people in indoor environments (up to 1.4~m/sec). The base consisted of a metallic, triangular-shaped structure where motors, batteries, computer and necessary electronics are embedded. In total, the robot weighed $22.3$ kg. Triskar had simultaneously and independently controlled rotational and translational motion capabilities thanks to three omnidirectional wheels actuated by a motor each. The robot's movement on flat floor was as free as the human which made it a good base for adversarial~\gls{pirg}s and, in particular, the one that we had designed. 

\input{chart/hardware_map.tex}

During the research progress, we have designed several versions of our robot, where the first two had an overall height of 85~cm, so comparable to that of a child players, but also acceptable for adult players. The first three versions adopted a Kinect\textsuperscript{\textregistered} sensor on top aimed at player tracking. Given the need to improve robustness, we have redefined the base by making structural changes to limit vibrations and improve stability of the sensors, which increased the overall height to 1 meter (2nd version). We also added new sensors such as planar laser scans needed to obtain reliable obstacle avoidance and localization (3rd and 4th version). Figure~\ref{fig:evolution} depicts our prototype evolution. Finally, on the fourth version we dropped the 3-D camera, deemed to be too much unreliable for player detection, and devised for this goal algorithms based on laser scans only. We kept the height in the same range as before, for the mentioned reasons, although it was no longer needed to hold Kinect\textsuperscript{\textregistered}. To justify the height and give a character to the robot, we added eyes and hair on top, which were appreciated by players. On the next section we expose some consideration regarding sensing.

\begin{figure}[ht]
      \centering
      \begin{subfigure}[b]{0.22\textwidth}
      	\centering
	    \framebox{\parbox{2.5cm}{\includegraphics[height=4cm,width=2.5cm]{images/03-foundation/triskar1}}}
	  	\caption{}
	  	\label{fig:evolution_a}
      \end{subfigure}
	 ~
	  \begin{subfigure}[b]{0.22\textwidth}
		\centering
	  	\framebox{\parbox{2.5cm}{\includegraphics[height=4cm,width=2.5cm]{images/03-foundation/triskar2}}}
	  	\caption{}\label{robot}
      \end{subfigure}
      ~
      \begin{subfigure}[b]{0.22\textwidth}
      	\centering
      	\framebox{\parbox{2.5cm}{\includegraphics[height=4cm,width=2.5cm]{images/03-foundation/triskar3}}}\label{newrobot}
      	\caption{}
      \end{subfigure}
      ~
      \begin{subfigure}[b]{0.22\textwidth}
	      \centering
	      \framebox{\parbox{2.5cm}{\includegraphics[height=4cm,width=2.5cm]{images/03-foundation/base4.png}}}\label{newrobot}
	      \caption{}
      \end{subfigure}
      \caption{Prototype evolution. a) First version having the Microsoft Kinect\textsuperscript{\textregistered} camera sensor support secured by steel cables in order to reduce vibration due to motion. b) Second version had improved stability by replacing the steel cables by rigid modular aluminum profiles. c) Third version had a completely redefined base including new electronics, thicker aluminum chassi, redesigned power distribution system and 2D lasers. This version also had a larger base compared to previous prototypes; d) Current version during demonstration at the Maker Faire 2018 in Rome (the European edition) from October 12th to 14th of 2018. A better placement of lasers had made the use of Kinect\textsuperscript{\textregistered} unnecessary since it allowed for full 360\textsuperscript{$\circ$} laser sensing coverage.}
      \label{fig:evolution}
\end{figure}

\subsection{Sensing}
\subsubsection{Microsoft Kinect\textsuperscript{\textregistered}\label{sec:kinectsec}}
In some phases of our development we have used the Microsoft Kinect\textsuperscript{\textregistered} sensor. It is a 3-D camera: in addition to providing an RGB image with its 1080p color camera, it also provides a depth map,  meaning that for every pixel of the depth image provided by the sensor, Kinect\textsuperscript{\textregistered} provides the distance from the sensor (see appendix~\ref{app:hard_appendix} for sensor specifications). This makes the it suitable for a variety of computer vision problems like background removal, blob detection, and people tracking.

\subsubsection{Laser scanners}\label{sec:lasers_hokuyo}
The robot was equipped with two laser scanners Hokuyo URG-04LX\footnote{\url{https://www.hokuyo-aut.jp/search/single.php?serial=165} accessed on \today}
%, shown in Figure~\ref{fig:hokuyo}
. These laser sensors perceive the range of obstacles on a plan with a field of view of $240^\circ$ and a resolution of $0.36^\circ$, the maximum detectable distance is $5.6m$ and they can be connected to the computer by means of a USB interface, being operated with a nominal voltage of 5V.

The Hokuyo URG-04LX consists of a compact stacked structure with a spindle motor and the actual scanner on top of it. The motor rotates a small transmission mirror that deflects the vertical laser beam coming from the top of the sensor into horizontal direction. This allows the laser beam to scan a planar area around the sensor with an opening angle of $240^\circ$. A second mirror below, the reception mirror, deviates the horizontal laser beam captured by a lens into vertical direction again.

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width=5cm]{images/03-foundation/hokuyo}
% 	\caption{ An Hokuyo URG-04LX laser scanner.}
% 	\label{fig:hokuyo} 
% \end{figure}

A full scan is performed every 100 ms. We mounted a laser scanner on each side of the lower chassis allowing for a $360^\circ$ coverage around the robot at a height of 30 cm from the floor.

\subsection{Structure \& Materials}
\subsubsection{Chassis}
The chassis of our robot was made entirely from modular aluminum profiles put together as to define the shape in figure~\ref{fig:aluminum_structure} (3rd prototype). The arrangement of the wheels, as to allow for the holonomic behavior, also allowed us to place batteries conveniently~(see figure~\ref{fig:batteries}) as well as the onboard computer and electronics~(see figure~\ref{fig:computer}). The careful placement of heavyweight elements, such as the lead-acid batteries, turned out to be important in our design since it impacted manoeuvrability. For instance, during test it occurred that, due to inertia, the robot would become vertically unstable when making quick turns or rapidly reacting to the human player's presence. 

The constraint of having the Microsoft Kinect\textsuperscript{\textregistered} sitting on top of a aluminum profile, as to allow maximum visibility from the sensor, moved the center of mass upwards making vertical balance worse. The void created between the wheels, in prototype versions three and four, elegantly provided a way to balance weights and make the robot's center of mass close enough to its base center. Figure~\ref{fig:evolution_a} documents the inefficient weight distribution in the first prototype. In the figure its possible to see the placement of one of the lead-acid batteries in the right side of the base, thus, making the weight distribution asymmetric.

Due to forces acting upon the robot during motion we have used software solutions for velocity smoothing that would increasingly bound speed when accelerating and decelerating. This, together with a proper weight distribution, rendered the robot's motion smooth, improving control, at the benefit of reducing the chances of mechanical breakdown caused by material stress.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.22\textwidth}
      	\centering
        \framebox{\parbox{2.5cm}{\includegraphics[height=4cm,width=2.5cm]{images/03-foundation/structure}}}
        \caption{}
        \label{fig:aluminum_structure}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.22\textwidth}
      	\centering
        \framebox{\parbox{2.5cm}{\includegraphics[height=4cm,width=2.5cm]{images/03-foundation/structureII}}}
        \caption{}
        \label{fig:batteries}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.22\textwidth}
      	\centering
        \framebox{\parbox{2.5cm}{\includegraphics[height=4cm,width=2.5cm]{images/03-foundation/structureIII}}}
        \caption{}
        \label{fig:computer}
    \end{subfigure}
    \caption{Structural details of our robot. The chassis is made of modular aluminum profiles from which rapid and easy prototyping is possible while retaining a good level of robustness. a) The full scale view of the robot (3rd version). b) a close-up view of one of the 2-D lasers and batteries (gray boxes). c) a close-up view of the vertical placement of computer (black box) and control board (red board).}
    \label{fig:structure_details}
\end{figure}

\subsection{Kinematics}
As said before, our robot had holonomic kinematics. Holonomy being referred to existing restrictions applied among translational axes. If a robot is holonomic with respect to $N$ dimensions, it's then capable of moving in any direction in any of the $N$ physical dimensions available to it. Our robot  matches the kinematics of a human player well, since it is possible to refer to humans as holonomic within two-dimensional space. Mathematical details regarding a 3-wheel omnidirectional robot is given in appendix~\ref{app:hard_appendix}.

\subsection{Power Supply}
Our robotic platform operated at a nominal voltage of 24V with two lead-acid 12V batteries connected in series. A DC-DC board was used to regulate the input voltage down to 12V, which was the nominal voltage for the onboard computer and Kinect\textsuperscript{\textregistered} device -- both being powered independently through the DC-DC. The motors, were powered directly from the 24V via a separate circuit with an independent turn on/off switch and emergence button. Having separate circuits with independent switches was necessary in order to allow for turning off the motors (in case of an emergence) independently of the electronics (computer, sensors and control board). The two lasers were powered via USB connection to the PC. The power  configuration allowed for a total time of 40 minutes before demanding battery replacement. Details regarding the batteries and electrical circuits are described in appendix~\ref{app:hard_appendix}.

\subsection{Computing \& Electronics}
\subsubsection{Onboard computer}\label{onboard pc}
For computing a Shuttle XPC Slim DH270 was used. The device has a $190 \times 165 \times 43$~mm steel case, and weights $1.3$~kg.  The armature presents two holes for Kensington Locks and numerous threaded holes (M3) at both sides allowing for an easy placement. The operating system used was~\textit{Ubuntu 16.04.3 LTS (64bit)}. The computer had an Intel Core i7 and a RAM DDR4 memory of 16 GB .

\subsubsection{Control boards}
\label{novacore}
The low-level motors actuation and their interface between the ROS system have been realized with the Nova Core modules based on STM32-chip~\cite{noauthor_nova_nodate}, which implement ready to use components to fulfill robot prototyping requirements with plug \& play approach.  
The provided modules allow to control different type of motors that can be modeled as a second order system, where the input is the voltage applied to the motor armature and output variable is the motor angular speed. Futher details about the deployed boards are presented in figure~\ref{fig:boards}.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
  \centering
      \includegraphics[width=3cm,height=2.5cm]{images/03-foundation/udc}
	\caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
  \centering
      \includegraphics[width=3cm,height=2.5cm]{images/03-foundation/io}
	\caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
  \centering
      \includegraphics[width=3cm,height=2.5cm]{images/03-foundation/usb}
	\caption{}
	\label{fig:usb_board}
  \end{subfigure}
  \caption{a) UDC board (1 per each motor) capable of driving motors up to 70 W, with torque, speed, and position closed loop control. General attributes: 5-28V supply; 3A max (5A peak); current sense; encoder input; limit switch input; 25 x 45 mm in size. b) IO board (1 per each motor): Integrate existing hardware into the real-time Nova Core bus with analog and digital signals. General attributes: 8 digital GPIO;  4 analog inputs; 2 analog outputs; 2 UART; 1 I2C; 1 SPI; 25 x 45 mm in size. c) USB board (used for data collection) Interface the real-time Nova Core bus with a computer and logs data to microSD memory. General attributes: USB connector;  UART connector; microSD card slot; rosserial support; 25 x 45 mm in size.}
  \label{fig:boards}
\end{figure}
%ANDY all the technical details and names of devices could go in an appendix, while I'll leave here a functional description of robot and sensors, giving the characteristics that are functionally relevant, e.g., speed, omnidirectionality, range of sensors, their data rate, ...

\subsection{Navigation}
\subsubsection{\glsdesc{slam}}\label{gmapping}
To create a map of the environment, our system used \textit{gmapping}\footnote{http://wiki.ros.org/gmapping accessed on \today}, a ROS package used for~\gls{slam} that estimates the map of the environment and the trajectory of the robot using a technique known as~\gls{rbpf}~\citep{grisettiyz_improving_2005} and the provided odometry and laser measurements. The procedure in\textit{gmapping} can be decomposed in two phases: \begin{inparaenum}[\itshape a\upshape)]\item update the robot's state using all available positioning related data. In our case, odometry and laser measurements was used; \item update the map of the environment using~\gls{rbpf}.\end{inparaenum}

The obtained map is an occupancy grid and it is represented as an image showing the blueprint of the environment. Once created, the integrated localization in ROS will be able to generate reference frame transforms from the map-frame to the robot odometry frame, correcting for estimated position in the environment. In Figure~\ref{fig:playground_map} an example of map is shown, with tower positions being clearly visible.

Considering only the playground (area of the rectangle delimited by four towers), however, introduce several problems to the localization algorithm. For instance, when the playground is put in a rectangular room with only the towers as ``furniture'' the symmetry of the environment does not possess enough descriptive features to infer the position unequivocally and localization is worsened. For this reason it was needed to include enough descriptive features around the playground borders to enhance the robot localization during the game. The map in figure~\ref{fig:playground_map} is a good example of a map with good features since the walls around the playground are ``jagged enough'' to distinguish the walls among themselves. 

\begin{figure}[h]
	\centering
	\framebox{\parbox{5cm}{\includegraphics[width=5cm]{images/03-foundation/playgroundmap}}}
	\caption{Map of the room with playground obtained with gmapping algorithm.} 
	\label{fig:playground_map}
\end{figure}

In particular, aiming at standardizing the map across different environments, reduce the number of times it had to be remade and filter out noise, we have decided to enclose the playground by ``walls'' made from fabric and supported by lightweight aluminum profiles joined together. A custom entrance made in one of the sides made up for the element breaking the symmetry and allowing for effective localization. This arrangement could easily be reused without having to remake the map. Enclosing the area was particular useful when collecting data since very often~\gls{pirg}s tend to gather a number of people around the playground which can compromise the environmental perception by the robot. Furthermore, it allowed us to reduce the complexity of tracking the player, as it is going to be explained later. 

\subsection{Tower navigation}
In this section we describe the algorithm that enables the robot to navigate from its current position to a target tower and its implementation. For this purpose, a point-to-point trajectory following approach has been considered. The proposed strategy is composed of three main part: sensor data analysis, obstacle detection and goal seeking. In general, with our navigation we wanted to provide a certain level of challenge to the human player while also trying to minimize behaviors that could lead to odometry errors and loss of localization in the map. The selection of the target tower takes into account the proximity relationship between the current position of the robot and that of the human player. The particular algorithm for that is going to be presented later as well as the mechanism for player tracking.

For the navigation, we focused in particular on avoiding wheel slippage during the initial acceleration phase. Generally, the selection of a target tower by the robot is performed such that the direct path leading to it is as free as possible considering the current player's position. 

During navigation, in case the human player is detected below a certain safety distance from the moving robot, the algorithm switches to the obstacle avoidance procedure described in section~\ref{sec:obt_avoidance}. As inputs to the navigation algorithm, we consider: the robot's estimated $x$ and $y$ position ($\hat{x}_R$, $\hat{y}_R$); target tower xy-coordinate ($x_G$, $y_G$) and a flag variable that assumes the value false if the robot is approaching an obstacle (\textsc{is\_safe}). The raw outputs are: the unsmoothed xy-linear velocity in the world frame ($\dot{\bar{x}}_R$, $\dot{\bar{y}}_R$) and a a flag variable that is true if the robot is close to a targeted tower (\textsc{near\_goal}). The procedure for generating velocity commands to the platform is detailed in algorithm~\ref{alg:pointopoint}. At each control loop call the algorithm publishes a new velocity command.

\begin{algorithm}[ht]
	\# define initial e final points when the robot receives the id of the targeted tower \;
	$x_d$ = ($\hat{x}_R$, $x_G$)\;
	$y_d$ = ($\hat{y}_R$, $y_G$)\;
	\# define the robot deviation from the required trajectory\;
	$\Delta x = x_d[1] - x_d[0]$\;
	$\Delta y = y_d[1] - y_d[0]$\;
	\# generates the direction of the motion based on the euclidian distance from goal\;
	goal\_distance$ = \sqrt{({\Delta x}^2 + {\Delta y}^2)}$\;
	$\alpha = $\textbf{atan2}$(\Delta x, \Delta y)$\;
	\# check if the robot is near its goal (this will trigger the obstacle avoidance behavior)\;
	\If{(goal\_distance < NEAR\_GOAL\_Treeshold)}{
		near\_goal = true\;
	}
	\# SAFETY CHECK: the controller will generate velocity commands only if the safety condition is satisfied. if safety condition is satisfied then: enable == 1 (player not too close to the robot);\;
	\If{(is\_safe = true)}{
		$\dot{\bar{x}}_R=V_\text{max}\cdot\cos(\alpha)$\;
		$\dot{\bar{y}}_R=V_\text{max}\cdot\sin(\alpha)$\;
	}
	
	\Return $\dot{\bar{x}}_R$, $\dot{\bar{y}}_R$, is\_safe 
	\caption{Point-to-Point navigation algorithm.}
	\label{alg:pointopoint}
\end{algorithm}

Prior to the execution by the low-level sub-systems that control the motors, the velocity vector $<\dot{\bar{x}}_R,\dot{\bar{y}}_R>$ output by algorithm~\ref{alg:pointopoint} need further processing. Specially, it needs to be transformed from the world reference frame (the frame in which towers are located in) to the robot reference frame (see transformation details in appendix~\ref{app:hard_appendix}).

Another aspect that had to be addressed was that of guarantying the decoupling between robot rotations and translations. The control of the robot orientation had to be completely independent from the xy-translation, since the robot in our scenario had to be able to track the movement of the human player while navigating towards the target towers. Mathematical detail of this process is once again available in appendix~\ref{app:hard_appendix}. With this approach, the transformed velocity set-points $\dot{\bar{x}}_R^m$ and $\dot{\bar{y}}_R^m$, dynamically change during the game and often the variation has a step-function shaped form that causes the low level actuation to react violently to make the robot reach the desired velocity at once -- especially when starting from the initial position with 0-velocity (see figures~\ref{vel14} and~\ref{wheel14}). This will result in the robot wheels undergoing in slippage due to the too sharp acceleration required to reach the velocity set-point. This is an undesired effect that is source of non-systematic odometry errors that may lead to instability and loss of localization. 

A number of tests have been performed to quantify the effect of wheel slippage during the initial robot acceleration phase. In these tests the robot linearly translates along $x$ and $y$ axis and receives the initial velocity set-point at $1m/s$ or $1.4m/s$. In figures~\ref{opti14},~\ref{amcl14},~\ref{poserror14},~\ref{opti14y},~\ref{amcl14y}, and~\ref{poserror14y} we can see the localization error introduced during such test.

To avoid this effect, a velocity smoother has been used in order to obtain a smooth velocity set-point control signal to be sent to the robot low-level actuation, towards avoiding wheels slippage during the acceleration phase. The inputs of the velocity smoother are the unsmoothed velocity set-points [$\dot{\bar{x}}_R^m$, $\dot{\bar{y}}_R^m$], some of the internal parameters such as maximum allowed velocity and maximum allowed acceleration can be changed online to modify the final response of the robot. The output are the final xy-velocity control signals [$\dot{x}_R^m$, $\dot{y}_R^m$] in robot reference frame that will be sent to the low level actuation.

The velocity smoother runs together with algorithm~\ref{alg:pointopoint}, receiving the output of this later as input. It basically pre-filters any incoming command input based on some acceleration constraints. Smoothing velocities are not just important for the reduction of localization errors, but is also an important factor in the reduction of material stress on the wheels, which can lead to mechanical breakdowns. In fact, during our research we have had a couple of mechanical breakdown due exactly to excessive acceleration. Such issues make up for important example of typical hardware concerns and issues on must deal with in the design of mobile agents for~\gls{pirg}.

\subsubsection{\glsdesc{oa}}\label{sec:obt_avoidance}
For in-game obstacle avoidance, a fuzzy-logic based reactive control has been designed to respond appropriately to sensory input. This was used principally because the human player could move completely unconstrained through the playground,~\ie proximity between human and robot could be sensed from any direction and in any moment during the game. Having fuzzy-logic rules for the decision making provided much soft navigation and stable control since a fuzzy approach can handle well the uncertainty when the sensed obstacle is at the boundary of sensing areas.

As first step, considering the laser sensing capabilities of our robot (see section~\ref{sec:lasers_hokuyo}), we have divided the rays cast from the laser into different discrete sensing areas around the platform. The information  contained in each laser scan message received from the ROS Hokuyo node (published in the topic \textit{/scan}) is composed of 1000 rays, each one of which carrying information about the distance and bearing angle of the sensed object w.r.t the sensor origin. The laser scan data was divided by indexes as follows:

\begin{itemize}
	\item \textbf{Rear Right:} indexes from 1 to 150;
	\item \textbf{Right:} indexes from 151 to 310;
	\item \textbf{Front Right:} indexes from 311 to 500;
	\item \textbf{Front Left:} indexes from 501 to 690;
	\item \textbf{Left:} indexes from 691 to 851;
	\item \textbf{Rear Left:} indexes from 850 to 1000.
\end{itemize}

For each area, the minimum distance to objects are calculated and a flag variable monitors whether an obstacle is within a given threshold distance (in our case $0.75$ meters). When this condition is verified, the fuzzy obstacle avoider takes the control from the trajectory navigation in order to handle the obstacle at hand. When the robot is close within a specified distance threshold from a target tower the obstacle avoider is disabled, thus, enabling it to tear down the towers instead of considering them as obstacles. In general, the output of the avoider is the tuple $(V_{x\_\text{avoider}}, V_{y\_ \text{avoider}})$ controlling the xy-translations.

when calculating the membership of an obstacle w.r.t. a sensory region, if the minimum distance of a sensing sector has index belonging to the fuzzy areas the ``crisp'' output values after defuzzyfication will be a mixture of the results of the rules associated to the two contiguous sectors. The membership functions shown in figure~\ref{indexesmmf} illustrates the idea. On the x-axis are reported the index values in a range from 1 to 1000, each one of it representing one of the 1000 laser's ray distance measurement that the system receives from each ROS /scan message.

For capturing the minimum detected distance in each sector the membership functions shown in figure~\ref{minmmf} had been defined. The x-axis ranges from 0 to $0.75$ meters (as said, the obstacle avoidance takes control of the robot's navigation when obstacles are detected below this threshold) the three labels \textit{``Close''}, \textit{``Far''}, and \textit{``Dontcare''} were used to define the intensity of the control action during the obstacle avoidance maneuver.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{images/03-foundation/indexesmmf}
	\caption{Membership functions (MMFs) for minimum detected distance \textit{index} in each sensing sector.}
	\label{indexesmmf} 
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{images/03-foundation/minmmf}
	\caption{Membership functions (MMFs) for minimum detected distance \textit{index} in each sensing sector.}
	\label{minmmf} 
\end{figure}

The system uses a Mamdani-type inference that requires the output membership functions to be fuzzy sets. After the aggregation process, \ie after evaluating the result of each rule, these results are combined to obtain a final result. There is a fuzzy set for each output variable that needs defuzzification.

The used defuzzification method was the ``Centroid'' method that returns the center of mass of the area under the curve, it can also be seen as a Center of Gravity and can be obtained with equation~\ref{centroid}:
\begin{equation}
U=\dfrac{\int_{\text{min}}^{\text{max}} u\cdot\mu(u) du}{\int_{\text{min}}^{\text{max}}\mu(u) du}
\label{centroid}
\end{equation}
where U is the center of gravity, $u$ is the output variable and $\mu(u)$ is the membership function after the accumulation, implemented as the maximum. Letting $\mu_A(u)$ and $\mu_B(u)$ be the membership functions for fuzzy sets A and B and for OR and AND operators are max and min, respectively.
\begin{align}
	\textbf{Accumulation method}\qquad max\lbrace\mu_A(u),\mu_B(u)\rbrace\\
	\textbf{OR (union)}\qquad max\lbrace\mu_A(u),\mu_B(u)\rbrace\\
	\textbf{AND (intersection)}\qquad min\lbrace\mu_A(u),\mu_B(u)\rbrace
\end{align}
As output membership functions, singletons have been selected. This enhances the efficiency of the defuzzification process because it greatly simplifies the computation required by the more general Mamdani method, since, instead of finding the centroid of a 2-D function by integrating across the two-dimensional function to find the centroid, the weighted average of a few data points is used as shown in equation~\ref{singletoncentroid}, where $p$ is the number of singletons.

\begin{equation}
U =	\dfrac{\sum_{i=1}^{p}\left[u_i,\mu_i\right]}{\sum_{i=1}^{p}\mu_i}
\label{singletoncentroid}
\end{equation} 

The singletons membership functions for $V_{x\_ \text{avoider}}$ and $V_{y\_ \text{avoider}}$ are reported in figure \ref{outmmf}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{images/03-foundation/outputmmf}
	\caption{MMfs for $V_{x\_ \text{avoider}}$ and $V_{y\_ \text{avoider}}$, the output velocities along xy-axis range from -0.5 m/s to 0.5 m/s}
	\label{outmmf} 
\end{figure}

A set of fuzzy rules (reported in appendix~\ref{app:hard_appendix})	has been implemented to obtain an overall obstacle avoidance behavior that has been inspired from the artificial potential fields approach. The rules have been designed to mimic a repulsive field that will drive the robot away from obstacles when these are encountered during navigation (figure~\ref{avoid1}). In particular, when the robot reaches the condition of proximity with the human player during the game, the set of fuzzy rules will cause the robot to drift away from him until the proximity condition is no more verified ($is\_safe$ equals true. Refer to algorithm~\ref{alg:pointopoint}). At this point, the robot will go back to the normal point-to-point navigation to reach the selected target tower.

Since the ultimate goal of the robot is to physically tear down one of the four towers that are placed in the playground, the fuzzy rules also implement an attractive effect (figure\ref{avoid2}) that is exerted by the target tower when the robot comes close to it  ($near\_goal == true$, again refer to algorithm~\ref{alg:pointopoint}).

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=5cm]{images/03-foundation/avoid1}
		\caption{}
		\label{avoid1} 
	\end{subfigure}
    ~
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=5cm]{images/03-foundation/avoid2}
		\caption{}
		\label{avoid2}
	\end{subfigure}
	\caption{a) Representation of the repulsive effect implemented by the fuzzy rules. b) Representation of the attractive effect implemented by the fuzzy rules. }
	\label{rulesbehavior}
\end{figure}

\subsection{Player perception \& tracking}
For opening the device, we have used libfreenect2\footnote{~\url{https://github.com/OpenKinect/libfreenect2} accessed on \today.}, a library for managing the Kinect\textsuperscript{\textregistered} device on Linux. We have then created a custom tracking node capable of estimating the player's position in two phases: First, color blob detection. Second, segmentation on the depth frame using \textit{region growing} algorithm for the purpose of detecting the player's body. Two features can be detected from this procedure: distance (relative to the robot) and contraction index. The first is calculated from the mean value of the depth pixels around the center of the blob. The latter is computed based on the subtraction of the occupied area with respect to the dimension of the bounding box that encompasses the segmented silhouette (see Figure~\ref{fig:segmenta}). This feature had been considered as a cue about the fact that the player was opening arms, so, possibly, actively participate to the game.

\begin{figure}[h]
  \centering 
  \begin{subfigure}[b]{0.3\textwidth}
		\centering
		\framebox{\parbox{3cm}{\includegraphics[width=3cm, height=3cm]{images/03-foundation/point_cloud}}}
		\caption{}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
		\centering
		\framebox{\parbox{3cm}{\includegraphics[width=3cm, height=3cm]{images/03-foundation/depth}}}
		\caption{}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
		\centering
		\framebox{\parbox{3cm}{\includegraphics[width=3cm, height=3cm]{images/03-foundation/segmentation}}}
		\caption{}
  \end{subfigure}
  \caption{Example of frames processed by our algorithm. a) Point cloud showing the center of the detected (light-purple) color blob. b) Depth frame. The number showed above the user correspond to his estimated distance relative to the robot (in meters). c) Segmentation results. The number above is the contraction index defined in the interval [0,1] and can be used as a measure of body contraction.}\label{fig:segmenta}
   \label{segmentacao}
\end{figure}